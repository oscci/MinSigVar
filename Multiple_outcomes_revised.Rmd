---
title: 'Using multiple outcomes in intervention studies:
  improving power while controlling type I errors'
author: "D. V. M. Bishop"
date: "13/11/2022"
output:
  word_document: default
  html_document: default
  pdf_document: default
bibliography: refs.bib
editor_options: 
  chunk_output_type: console
---



<!-- To do:  

For figures: https://rmd4sci.njtierney.com/figures-tables-captions-.html

Check the terminology - familywise error rate, adjusted alpha etc. Need to be consistent throughtout.  
Make figures showing the correlation structures that are considered.  
Read earlier Vickerstaff.  
? Situation if only some variables have a real effect. 
Do we need to now vary effect size by variables as well as correlation structure?
Suppose we have 2 factors and one has big ES and one has none. If you use multiple outcomes and MEff, you might reduce chances of finding the effect?
This is case where you are really going to need to replicate to see whether the pattern is chance or genuine?
But first let's just do the power stuff with the simple case, and compare with PCA
-->



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Note to users: 
# Simulated data for 10,000 runs per condition, as well as other .csv files used here can be downloaded from https://osf.io/5t4se/files/

require(tidyverse)
require(flextable)
require(MASS)
require(pwr)
require(graphics)
require(effsize)
require(tidyr)
require(ggplot2)
require(DescTools) #for Fisher Z
require(here)
require(kableExtra)
require(knitr)
require(reshape2) #for melt
fignum <- 0
tabnum <-0
set.seed(20) #nb this seed was *not* set for simulated data reported in the ms

```

```{r numformat,echo=F}
#Format numbers so they have same n decimal places, even if zero at end
#This returns a string

numformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  return(newnum)
}
```


## Abstract

### Background   

The CONSORT guidelines for clinical trials recommend use of a single primary outcome, to guard against the raised risk of false positive findings when multiple measures are considered. It is, however, possible to include a suite of multiple outcomes in an intervention study, while controlling the familywise error rate, if the correlation between outcomes is known. The MEff statistic is a relatively simple approach that is well-suited to this purpose, but is not well-known outside genetics. Where multiple outcomes are selected to index a common latent factor that is targeted by intervention, use of MEff can substantially increase power of an intervention study. 

### Methods  

Data were simulated for an experimental evaluation of an intervention, with a given sample size (N), effect size (E) and correlation matrix for a suite of outcomes (**R**). Using the variance of eigenvalues from the correlation matrix, we can compute MEff, the effective number of variables that the alpha level should be divided by to control the familywise error rate. Various scenarios are simulated to consider how MEff is affected by the pattern of pairwise correlations within a set of outcome measures. Simulated data is used to compare the power of this approach compared to Bonferroni correction, or a principal component analysis. 

### Results  

In certain situations, notably when the sample size is around 50-80 per group and there is a moderate correlation between outcome measures (.4-.5) and medium effect size (.3-.5), statistical power can be increased by inclusion of multiple outcome measures, while using MEff to maintain the familywise error rate at .05. 

### Conclusions   

Where it is feasible to have a suite of moderately correlated outcome measures, then Meff can provide a more efficient approach than reliance on a single primary outcome measure in an intervention study. In effect, it builds in an internal replication to the study. With appropriate control of the alpha level using Meff, the researcher can identify a true intervention effect if any one of a set of outcome measures gives a statistically significant result. This approach can also be used to evaluate published intervention studies, if the correlation between outcome measures can be estimated. 

## Keywords  
intervention, methodology, statistics, correlated outcomes, power, familywise error rate, multiple comparisons
  

## The case against multiple outcomes

The CONSORT guidelines for clinical trials [@moher2010] are very clear on the importance of having a single primary outcome:  
_All RCTs assess response variables, or outcomes (end points), for which the groups are compared. Most trials have several outcomes, some of which are of more interest than others. The primary outcome measure is the pre-specified outcome considered to be of greatest importance to relevant stakeholders (such as patients, policy makers, clinicians, funders) and is usually the one used in the sample size calculation. Some trials may have more than one primary outcome. Having several primary outcomes, however, incurs the problems of interpretation associated with multiplicity of analyses and is not recommended._  

This advice often creates a dilemma for the researcher: in many situations there are multiple measures that could plausibly be used to index the outcome. If we have several outcomes and we would be interested in improvement on any measure, then we need to consider the __familywise error rate__, i.e. the probability of at least one false positive in the whole set of outcomes. For instance, if want to set the false positive rate,  alpha to .05,  and we have six independent outcomes, none of which is influenced by the intervention, the probability that __none__ of the tests of outcome effects is significant will be .95^6, which is .735. Thus the probability that __at least one__ outcome is significant, the familywise error rate, is 1-.735, which is .265. In other words, in about one quarter of studies, we would see a false positive when there is no true effect.

A common solution is to apply a Bonferroni correction by dividing the alpha level by the number of outcome measures - this example .05/6 = .008. This way the familywise error rate is kept at .05. But this is over-conservative if, as is usually the case, the various outcomes are intercorrelated.  

To illustrate the problem with a realistic example, suppose we are reading a report of a behavioural intervention that is designed to improve language and literacy, and there are 6 measures where we might plausibly expect to see some benefit. The researchers report that none of the outcomes achieves the Bonferroni-adjusted significance criterion of p < .008, but two of them reach significance at p < .05. Should we dismiss the trial as showing no benefit?  We can use the binomial theorem to check the probability of obtaining this result or a more extreme one if the null hypothesis is true and the measures are independent: it is `r round(binom.test(2,6,.05)$p.value,3)`,  below the 5% alpha level. But what if the measures are intercorrelated? That is often the case: indeed, it would be very unusual for a set of outcome measures to be independent. A thought experiment helps here. Suppose we had six measures that were intercorrelated at .95 - in effect they would all be measures of the same thing, and so if there was a real effect, most of the measures should show it. Extending this logic in a more graded way, the higher the correlation between the measures, the more measures would need to reach the original alpha level to maintain the familywise significance level below .05. At the other extreme, if the correlations between measures were all zero, then the Bonferroni correction would be appropriate. 

Alternative methods have been developed to address this issue. One approach is to adopt some process of data reduction, such as extracting a principal component from the measures that can be used as the primary outcome.  Alternatively, a permutation test can be used to derive exact probability of an observed pattern of results. Neither approach, however, is helpful if the researcher is evaluating a published paper where an appropriate correction has not been made. These could be cases where no correction is made for multiple testing, risking a high rate of false positives, or where Bonferroni correction has been applied despite using correlated outcomes, which will be overconservative in rejecting the null hypothesis. The goal of the current article is to provide some guidance for interpretation of published papers where the raw data are not available for recomputation of statistics. 

In a review of an earlier version of this paper, Sainani (2021) pointed out that the MEff statistic, originally developed in the field of genetics by Chevrud (2001) and Nyholt (2004), provided a simple way of handling this situation. With this method, one computes eigenvalues from the correlation matrix of outcomes, which reflect the degree of intercorrelation between them. The mathematical definition of an eigenvalue can be daunting, but an intuitive sense of how it relates to correlations can be obtained by considering the cases shown in Table 1.  This shows how eigenvalues vary with the correlation structure of a matrix, using an example of six outcome measures. The number of eigenvalues, and the sum of the eigenvalues, is identical to the number of measures. Let us start by assuming a matrix in which all off-diagonal values are equal to r.  It can be seen that when the correlation is zero, each eigenvalue is equal to one, and the variance of the eigenvalues is zero.  When the correlation is one, the first eigenvalue is equal to six, all other eigenvalues are zero, and the variance of the eigenvalues is six. As correlations increase from .2 to .8, the size of the first eigenvalue increases, and that of the other eigenvalues decreases.  


```{r makemat,include=F}
demoeigens<-function(nvar,cor){
m<-matrix(rep(cor,nvar*nvar),nrow=nvar)
diag(m)<-1
evs = eigen(m)$values

return(evs)
}
```

```{r makemat2,include=F}
#have correls split into two blocks: not currently used
demoeigens2<-function(nvar,cor){
m<-matrix(rep(cor,nvar*nvar),nrow=nvar)
diag(m)<-1
startc1<-1
endc1<-(nvar/2)
startc2<-(nvar/2)+1
endc2<-nvar
m[startc1:endc1,startc2:endc2]<-0
m[startc2:endc2,startc1:endc1]<-0
evs = eigen(m)$values
meancor<-mean(m[upper.tri(m)])

return(list(evs,meancor))
}
```

```{r eigendemo,include=F}
#This chunk creates Table 1
nvar=6
mycor<-seq(from=0,to=1,by=.2)
evstab<-data.frame(matrix(NA,ncol=(5+nvar),nrow=(2*length(mycor)-1)))
colnames(evstab)<-c('r',paste0('Eigen',1:nvar),'Var','MEff','AlphaMEff','Mean_corr')
evstab[,1]<-c(mycor,paste0(mycor[2:length(mycor)],'/',mycor[2:length(mycor)]))
thisrow<-0
for(cor in mycor){
  thisrow<-thisrow+1
  evs<-demoeigens(nvar,cor)
  evstab[thisrow,2:(1+nvar)]<-round(evs,2)
  evstab$Var[thisrow]<-var(evs)

}

  evstab$MEff<-1+(nvar-1)*(1-evstab$Var/nvar)
  evstab$AlphaMEff<-round(.05/evstab$MEff,3)
  evstab[,2:(1+nvar)]<-numformat(evstab[,2:(1+nvar)],1)
  thistab<- evstab[1:6,1:(ncol(evstab)-1)]
ft1<-flextable(thistab)
ft1<- fit_to_width(ft1, max_width =8 )
ft1<-set_caption(ft1,'Eigenvalues, MEff and AlphaMEff with 6 outcome variables')

  #we also add to the table the case where there are 2 separate factors, but this is not included in Table 1
for(cor in mycor[2:length(mycor)]){
  thisrow<-thisrow+1
  evs<-demoeigens2(nvar,cor) #this time we have a list with evs and the computed corr
  evstab[thisrow,2:(1+nvar)]<-round(evs[[1]],2)
  evstab$Var[thisrow]<-var(evs[[1]])
  evstab$Mean_corr[thisrow]<-evs[[2]]

}



  evstab$MEff<-1+(nvar-1)*(1-evstab$Var/nvar)
  evstab$AlphaMEff<-round(.05/evstab$MEff,3)

  #move the Meancorr into 2nd column
  nc<-ncol(evstab)
  evstab<-evstab[,c(1,nc,2:(nc-1))]
  
  write.csv(evstab,'EVStab.csv',row.names=F)

  
```

`r ft1`  
In Table 1, r is the intercorrelation between the six outcomes, Eigen1 - Eigen6 are the eigenvalues, and Var is the variance of the six Eigenvalues, which is used to compute MEff (the effective number of comparisons) from the formula:  
   MEff = 1 + (N-1)*(1-(Var(Eigen)/N)  
where N is the number of outcome measures, and Eigen is the set of N eigenvalues. 

This value is then used to compute the corrected alpha level, AlphaMEff. Assuming we set alpha to .05, AlphaMEff is .05 divided by MEff. One can see that this value is equivalent to the Bonferroni-corrected alpha (.05/6) when there is no correlation between variables, and equivalent to .05 when all variables are perfectly correlated. 

Derringer (2018) provided a useful tutorial on MEff, noting that it is not well-known outside the field of genetics, but is well-suited to the field of psychology. Her preprint includes links to R scripts for computing MEff and illustrates their use in three datasets. 

These resources will be sufficient for many readers interested in using MEff, but readers may find it useful to have a look-up table for the case when they are evaluating existing studies.  The goal of this paper is two-fold:  

A. To consider how inclusion of multiple outcome measures affects statistical power, relative to the case of a single outcome, when appropriate correction of the familywise error rate is made using MEff. Results from MEff are compared with use of principal components analysis (PCA) and Bonferroni correction. 
B. To provide a look-up table to help evaluate studies with multiple outcome measures, without requiring the reader to perform complex statistical analyses. 

These goals are achieved in three sections below:

1. Power to detect a true effect using MEff is calculated from simulated data for a range of values of sample size (N), effect size (E) and intercorrelation between outcomes (**R**)
2. A lookup table is provided that gives values of MEff, and associated adjusted alpha-levels for different set sizes of outcome measures, with mean pairwise correlation varying from 0 to 1 in steps of .1.
3. Use of the lookup table is shown for real-world example of application of MEff using a published  dataset. 

## Alternative approach, MinNVar  
In the original version of this manuscript, an alternative approach, MinNVar, was proposed, in which the focus was on the _number_ of outcome variables achieving a conventional .05 level of significance. As noted by reviewers, this has the drawback that it could not reflect continuous change in probability levels, because it was based on integer values (i.e. number of outcomes). This made it overconservative in some cases, where adopting the MinNVar approach gave a familywise error rate well below .05. One reason for proposing MinNVar was to provide a very easy approach to evaluating studies that had multiple outcomes, using a lookup table to check the number of outcomes needed, depending on overall correlation between measures. However, it is equally feasible to provide lookup tables for MEff, which is preferable on other grounds, and so MinNVar is not presented here; interested readers can access the first version of this paper to evaluate that approach. 

## Use of one-tailed p-values    
In all the analyses and simulations described here, one-tailed tests are used. Two-tailed p-values are far more common in the literature, perhaps because one-tailed tests are often abused by researchers, who may switch from a two-tailed to a one-tailed p-value in order to nudge results into significance.  

This is unfortunate because, as argued by Lakens (2016), provided one has a directional hypothesis, a one-tailed test is more efficient than a two-tailed test. It is a reasonable assumption that in intervention research, which is the focus of the current paper, the hypothesis is that an outcome measure will show improvement. Of course, interventions can cause harms, but, unless those are the focus of study, we have a directional prediction for improvement. 



## Methods  
Correlated variables were simulated using in the R programming language [@rcoreteam2020]. The script to generate and analyse simulated data is available on !!!!!.  For each model specified below, 1000 simulations were run. Note that to keep analysis simple, a single value was simulated for each case, rather than attempting to model change.  Data for the two groups were generated by the same process, except that an effect size, E, was added to scores of the intervention group, I, but not to the control group, C. Scores of the two groups were compared using a one-tailed t-test for each run.

Power was computed for different levels of effect size (E),  correlation between outcomes (**R**) and sample size per group (N) for the following methods:  

a) Bonferroni-corrected data: Proportion of runs where p was less than the Bonferroni-corrected value for at least one outcome.
b) MEff-corrected data: Proportion of runs where p was less than AlphaMeff value for at least one outcome. 
c) Principal component analysis (PCA): Proportion of runs where p was below .05 when groups I and C were compared on scores on the first principal component of PCA. 

### Method for simulating outcomes  

Simulating multivariate data forces one to consider how to conceptualise the relationship between an intervention and multiple outcomes.  Implicit in the choice of method is an underlying causal model that includes mechanisms that lead measures to be correlated. 

In the simulation, outcomes were modelled as indicators of one or more underlying latent factors, which mediate the intervention effect. This can be achieved by first simulating a latent factor, with an effect size of either zero, for group C, or E for group I. Observed outcome measures are then simulated as having a specific correlation with the latent variable - i.e. the correlation determines the extent to which the outcomes act as indicators of the latent variable. This can be achieved using the formula:  
$$
r * L + \sqrt{1-r^2} * E 
$$
where r is the correlation between latent variable (L) and each outcome, and L is a vector of random normal deviates that is the same for each outcome variable, while E (error) is a vector of random normal deviates that differs for each outcome variable. Note that when outcome variables are generated this way, the mean intercorrelation between them will be r^2^.  Thus if we want a set of outcome variables with mean intercorrelation of .4, we need to specify r in the formula above as sqrt(r) = .632. Furthermore, the effect size for the simulated variables will be lower than for the latent variable: to achieve an effect size, E, for the outcome variables, it is necessary to specify the effect size for the latent variable, E~l~, as E/sqrt(r). 

Note that the case where r = 0 is not computable with this method - i.e. it is not possible to have a set of outcomes that are indicators of the same latent factor but which are uncorrelated. The lowest value of r that was included was r = .2. 

Ths initial simulation,designated as Model L1, treated all outcome measures as equivalent. In practice, of course, we will observe different effect sizes for different outcomes, but in these models, this is purely down to the play of chance: all outcomes are indicators of the same underlying factor, as shown in the heatmap in Figure 1A.  

In two additional models, rather than being indicators of the same uniform latent variable, the outcomes correspond to different latent constructs. This would correspond to the kind of study described by Vickerstaff et al (2021), where an intervention for obesity included outcomes relating to  weight and blood glucose levels.  Following suggestions by Sainani (2021), a set of simulations was generated to consider relative power of different methods when there are two underlying latent factors that generate the outcomes. In Model 2 (Figure 1), there are two independent latent factors, both affected by intervention. In Model 3 (Figure 1), the intervention only influences the first latent factor. The computational approach was the same as for model 1, but with two latent variables, each used to generate a block of variables. The two latent factors are uncorrelated.

![Figure 1. Models for data generation. Heatmap where depth of colour corresponds to strength of association. A diagonal line through a latent factor indicates it is not related to intervention] ('Figures/ppt heatmap try.png')  



## Results  



## 1. Power calculations.
 
<!--Simulated data is created using Data_Simulation_L.Rmd.  
This is based on the original MinNVar approach but allows for cases where outcomes come from different blocks
The filename is saved as "simulated_pMEff_L1.csv" for method 1
The script 'Data_simulation_goodL.Rmd' generates the simulated data, the summary values for power, and the figures. Figures are in the Figures folder.
-->

Sample plots comparing power for Bonferroni correction, MEff and PCA are shown for sample size of 50 per group in Figures 2 to 4.  Plots for smaller (N = 20) and larger (N =  80)  sample sizes are available online, and show the same basic pattern.

Figure 2 shows the simplest situation when there are between 2 and 8 outcome measures, all of which are derived from the same latent variable (Method 1). Different levels of intercorrelation between the outcomes (ranging from .2 to .8 in steps of .2) are shown in columns.  

![Figure 2. Method 1, 50 per group: Power in relation to number of outcome measures (N outcomes), intercorrelation between outcomes (column headers), type of Correction, and Effect size. The square, circle and triangle symbols represent the power for a single outcome measure with effect size .3, .5 and .8 respectively.] ('Figures/Method_1_power_N_50.jpg')

Several points  emerge from inspection of this figure;  first, when intercorrelation between measures is low to medium (.2 to .4), power increases as the number of outcome measures increases. Furthermore, the power is greater when PCA is used than when MEff or Bonferroni correction is applied.  MEff is generally somewhat better-powered than Bonferroni, and Bonferroni has lower power than a single outcome measure when there is a large number of highly intercorrelated outcome measures.  

In practice, it may be the case that outcome measures are not all reflective of a common latent factor.  Figure 3 shows results from Method 2, where outcome measures form two clusters, each associated with a different latent factor (see Figure 1). Here both latent factors are associated with improved outcomes in the intervention group. 

![Figure 3. Method 2: 50 per group. Power in relation to number of outcome measures (N outcomes), intercorrelation between outcomes (column headers), type of Correction, and Effect size. The square, circle and triangle symbols represent the power for a single outcome measure with effect size .3, .5 and .8 respectively.] ('Figures/Method_2_power_N_50.jpg')


```{r fig3, fig.align = 'center', fig.cap = "Figure 3. Method 2: 50 per group. Power in relation to number of outcome measures (N outcomes), intercorrelation between outcomes (column headers), type of Correction, and Effect size. The square, circle and triangle symbols represent the power for a single outcome measure with effect size .3, .5 and .8 respectively."}
knitr::include_graphics(here::here('figures','Method_2_power_N_50.jpg'))
```

Once again, power increases with number of outcomes when there are low to modest intercorrelations between outcomes. For this method, PCA no longer has such a clear advantage. This makes sense, given that PCA will attempt to derive a single factor, when the underlying structure contains two independent factors.  

![Figure 4. Method 3: 50 per group. Power in relation to number of outcome measures (N outcomes), intercorrelation between outcomes (column headers), type of Correction, and Effect size. The square, circle and triangle symbols represent the power for a single outcome measure with effect size .3, .5 and .8 respectively.] ('Figures/Method_3_power_N_50.jpg') 

Figure 4 shows equivalent results for Method 3, where we have a mixture of two types of outcome, one of which is influenced by intervention, and the other is not. This complicates calculation of power for a single variable, since, power will depend on whether we select one of the outcomes that is influenced by intervention or not. The symbols in Figure 4 show average power, assuming we could select either type of outcome with equal frequency.   We see that in this situation, MEff is clearly superior to PCA.  


## 2. Deriving a lookup table. 



```{r makeMEff,include=F}
makeMEff<-function(allN,allr,method=1){
#the default method is 1, which is when all off-diagonal correlations are equal

MEfftab<-data.frame(matrix(NA,nrow=length(allr),ncol=1+length(allN))) #data frame to hold results
colnames(MEfftab)<-c('corr',paste0('N',allN))
MEfftab[,1]<-allr
alphatab<-MEfftab #copy the output table to make another table for alpha values
comprtab<-MEfftab #used to hold computed off diagonal means - used to check outputs for methods 2-3
myrow<-0
mycol<-0
for (i in allN){
  mycol<-mycol+1
  myrow<-0
  for (j in allr){
    myrow<-myrow+1
    
    mymat<-matrix(j,nrow=i,ncol=i)#correlation matrix with all values at j
    diag(mymat) <- 1
    
if(method==2){
#if method is 2, then offdiagonal values across sets are zero
# we assume we have even number for N, and make 2 sets
      set1<-1:round(i/2,0)
      set2<-(1+max(set1)):i

      for(k in set1){ #across the sets the correlation is zero
        for (l in set2){
          mymat[k,l]<-0
          mymat[l,k]<-0
        }
      }
}

n<-dim(mymat)[1]
uppert<-mymat[upper.tri(mymat)]
#compmean<-mean(FisherZ(uppert)) #computed mean for upper triangle
compmean<-mean(uppert)



    evs = eigen(mymat)$values
    eff = 1 + (i - 1) * (1 - var(evs) / i)
    MEfftab[myrow,(1+mycol)]<-round(eff,3)
    comprtab[myrow,(1+mycol)]<-round(compmean,3)
 }
}
alphatab<-MEfftab
alphatab[,2:ncol(alphatab)]<-round(.05/alphatab[,2:ncol(alphatab)],3)
return(list(MEfftab,alphatab,comprtab))
}
```

```{r runmakeMEffv1,include=F}
allN<-2:12
allr<-seq(0,1,.1)

myout<-makeMEff(allN,allr,method=1)
MEfftab1<-myout[[1]]
alphatab1<-myout[[2]]
ft2<-flextable(alphatab1)
ft2<- fit_to_width(ft2, max_width =8 )
ft2<-set_caption(ft2,'AlphaMEff for different correlation values with 12 outcome variables')

```

`r ft2`  

Table 2 shows corrected alpha values based on MEff, varying according to the correlation between outcome measures, and the number of outcome measures in the study. Of course, the problem for the researcher is to estimate the intercorrelation between outcome measures if this is not known. 

It is unrealistic to assume that there will be a uniform intercorrelation between outcome measures in the population.  Nevertheless, further simulations showed that values for MEff are reasonably consistent for different correlation matrices that all have the same average off-diagonal correlation. Consider, for instance, the correlations between 4 variables shown in Figure 1 for Method 2.  Within the blocks V1-V2 and V3-V4 the intercorrelation is __r__, but between blocks the intercorrelation is zero. There are six off-diagonal correlations and the mean off-diagonal is (2 * __r__) / 6. For instance, if __r__ equals .5, then the mean off-diagonal value is .167. We can compare MEff for Method 2 with the MEff obtained in Method 1, if all off-diagonal correlations are .167 - this exercise shows that they are similar, as shown in Table 3. 

In other words, if estimating MEff from existing data, it is reasonable to base the estimate on the __average__ off-diagonal correlation, regardless of the variation in correlations across the outcomes. 


## Application to a real example


## Discussion

Some interventions are expected to affect a range of related processes.  In such cases, the need to specify a single primary outcome tends to create difficulties, because it is often unclear which of a suite of outcomes is likely to show an effect. Note that the MEff approach does not give the researcher free rein to engage in p-hacking: the larger the suite of measures included in the study, the lower the adjusted alpha will be. It does, however, remove the need to put all one's eggs in one basket by pre-specifying one measure as the primary outcome. 

A second advantage is that in effect, by including multiple outcome measures, one can improve the efficiency of a study, in terms of the trade-off between power and familywise errors. A set of outcome measures may be regarded as imperfect proxy indicators of an underlying latent construct, so we are in effect building in a degree of within-study replication if we require that more than one measure shows the same effect in the same direction before we reject the null hypothesis. 

A possible disadvantage of using the MEff over principal components is that this approach is likely to tempt researchers to interpret specific outcomes that fall below the .05 threshold as meaningful. They may be, of course, but this simulation demonstrates that when we create a suite of outcomes that differ only by chance, it is common for only a subset of them to reach the significance criterion. Any recommendation to use MEff should be accompanied by a warning that a suite of outcomes should be selected as representative of the underlying construct the intervention is designed to influence, in effect serving as replicate measures, all of which should be equally promising as indicators of an intervention effect. If a subset of outcomes show an effect of intervention, this could be due to chance. It would be necessary to run a replication to have confidence in a particular pattern of results. 

It is also worth noting that results obtained with this approach depend crucially on assumptions embodied in the simulation that is used to derive predictions. Outcome measures simulated here are normally distributed, and uniform in their covariance structure.  It would be of interest to evaluate MEff in datasets with different variable types, such as those used by Vickerstaff et al (2021) that included binary as well as continuous data, as well as modeling the impact of missing data. 


Compared to most other approaches, MEff is relatively simple. It could potentially be used to reevaluate published studies that report multiple outcomes but may not have been analysed optimally, provided we have some information on the average correlation between outcome measures. 

In sum, the veto on use of multiple outcomes in intervention studies does not lead to optimal study design. Inclusion of several related outcomes can increase statistical power, without increasing the false positive rate, provided appropriate correction is made for the multiple testing. 



# Table titles  


# Figure titles  



# Notes  

The script is available on https://github.com/oscci/MinSigVar.

# Competing interests
No competing interests were disclosed.  

# References  




