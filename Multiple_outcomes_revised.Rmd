---
title: 'Using multiple outcomes in intervention studies:
  tables to help estimate the trade-off between power and type I errors'
author: "D. V. M. Bishop"
date: "22/09/2022"
output:
  word_document: default
  html_document: default
  pdf_document: default
bibliography: refs.bib
editor_options: 
  chunk_output_type: console
---

<!-- To do:  
Check the terminology - familywise error rate, adjusted alpha etc. Need to be consistent throughtout.  
Make figures showing the correlation structures that are considered.  
Read earlier Vickerstaff.  
? Situation if only some variables have a real effect. 
Do we need to now vary effect size by variables as well as correlation structure?
Suppose we have 2 factors and one has big ES and one has none. If you use multiple outcomes and MEff, you might reduce chances of finding the effect?
This is case where you are really going to need to replicate to see whether the pattern is chance or genuine?
But first let's just do the power stuff with the simple case, and compare with PCA
-->



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Note to users: 
# Simulated data for 10,000 runs per condition, as well as other .csv files used here can be downloaded from https://osf.io/5t4se/files/

require(tidyverse)
require(flextable)
require(MASS)
require(pwr)
require(graphics)
require(effsize)
require(tidyr)
require(ggplot2)
require(DescTools) #for Fisher Z
require(here)
require(kableExtra)
require(knitr)
fignum <- 0
tabnum <-0
set.seed(20) #nb this seed was *not* set for simulated data reported in the ms

```

```{r numformat,echo=F}
#Format numbers so they have same n decimal places, even if zero at end
#This returns a string

numformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  return(newnum)
}
```


## Abstract

### Background   

The CONSORT guidelines for clinical trials recommend use of a single primary outcome, to guard against the raised risk of false positive findings when multiple measures are considered. It is, however, possible to include a suite of multiple outcomes in an intervention study, while controlling the familywise error rate, if the correlation between outcomes is known. The MEff statistic is a relatively simple approach that is well-suited to this purpose, but is not well-known outside genetics. 

### Methods  

Data were simulated for an experimental evaluation of an intervention, with a given sample size (N), effect size (E) and correlation matrix for a suite of outcomes (**R**). Using the variance of eigenvalues from the correlation matrix, we can compute MEff, the effective number of variables that the alpha level should be divided by to control the familywise error rate. Various scenarios are simulated to consider how MEff is affected by the pattern of pairwise correlations within a set of outcome measures. Simulated data is used to compare the power of this approach compared to using a single outcome or a principal component analysis. 

### Results  

In certain situations, notably when the sample size is around 50-80 per group and there is a moderate correlation between outcome measures (.4-.5) and medium effect size (.3-.5), statistical power can be increased modestly by inclusion of multiple outcome measures, while using MEff to maintain the familywise error rate at .05. 

### Conclusions   

Where it is feasible to have a suite of moderately correlated outcome measures, then Meff can provide a more efficient approach than reliance on a single primary outcome measure in an intervention study. In effect, it builds in an internal replication to the study. With appropriate control of the alpha level using Meff, the researcher can identify a true intervention effect if any one of a set of outcome measures gives a statistically significant result. This approach can also be used to evaluate published intervention studies, if the correlation between outcome measures can be estimated. 

## Keywords  
intervention, methodology, statistics, correlated outcomes, power, familywise error rate, multiple comparisons
  

## The case against multiple outcomes

The CONSORT guidelines for clinical trials [@moher2010] are very clear on the importance of having a single primary outcome:  
_All RCTs assess response variables, or outcomes (end points), for which the groups are compared. Most trials have several outcomes, some of which are of more interest than others. The primary outcome measure is the pre-specified outcome considered to be of greatest importance to relevant stakeholders (such as patients, policy makers, clinicians, funders) and is usually the one used in the sample size calculation. Some trials may have more than one primary outcome. Having several primary outcomes, however, incurs the problems of interpretation associated with multiplicity of analyses and is not recommended._  

This advice often creates a dilemma for the researcher: in many situations there are multiple measures that could plausibly be used to index the outcome. If we have several outcomes and we would be interested in improvement on any measure, then we need to consider the __familywise error rate__, i.e. the probability of at least one false positive in the whole set of outcomes. For instance, if want to set the false positive rate,  alpha to .05,  and we have six independent outcomes, the probability that __none__ of the outcomes is significant will be .95^6, which is .735. Thus the probability that __at least one__ outcome is significant is 1-.735, which is .265.

A common solution is to apply a Bonferroni correction by dividing the alpha level by the number of outcome measures - this example .05/6 = .008. But this is over-conservative if, as is usually the case, the various outcomes are intercorrelated.  

To illustrate the problem with a realistic example, suppose we are reading a report of a behavioural intervention that is designed to improve language and literacy, and there are 6 measures where we might plausibly expect to see some benefit. The researchers report that none of the outcomes achieves the Bonferroni-adjusted significance criterion of p < .008, but two of them reach significance at p < .05. Should we dismiss the trial as showing no benefit?  We can use the binomial theorem to check the probability of obtaining this result or a more extreme one if the null hypothesis is true and the measures are independent: it is `r round(binom.test(2,6,.05)$p.value,3)`,  below the 5% alpha level. But what if the measures are intercorrelated? That is often the case: indeed, it would be very unusual for a set of outcome measures to be independent. A thought experiment helps here. Suppose we had six measures that were intercorrelated at .95 - in effect they would all be measures of the same thing, and so if there was a real effect, most of the measures should show it. Extending this logic in a more graded way, the higher the correlation between the measures, the more measures would need to reach the original alpha level to maintain the familywise significance level below .05. At the other extreme, if the correlations between measures were all zero, then the Bonferroni correction would be appropriate. 

Alternative methods have been developed to address this issue. One approach is to adopt some process of data reduction, such as extracting a principal component from the measures that can be used as the primary outcome.  Alternatively, a permutation test can be used to derive exact probability of an observed pattern of results. Neither approach, however, is helpful if the researcher is evaluating a published paper where an appropriate correction has not been made. These could be cases where no correction is made for multiple testing, risking a high rate of false positives, or where Bonferroni correction has been applied despite using correlated outcomes, which will be overconservative in rejecting the null hypothesis. The goal of the current article is to provide some guidance for interpretation of published papers where the raw data are not available for recomputation of statistics. 

In a review of an earlier version of this paper, Sainani (2021) pointed out that the MEff statistic, originally developed in the field of genetics by Chevrud (2001) and Nyholt (2004), provided a simple way of handling this situation. With this method, one computes eigenvalues from the correlation matrix of outcomes, which reflect the degree of intercorrelation between them. The mathematical definition of an eigenvalue can be daunting, but an intuitive sense of how it relates to correlations can be obtained by considering the cases shown in Table 1.  This shows how eigenvalues vary with the correlation structure of a matrix, using an example of six outcome measures. The number of eigenvalues, and the sum of the eigenvalues, is identical to the number of measures. Let us start by assuming a matrix in which all off-diagonal values are equal to r.  It can be seen that when the correlation is zero, each eigenvalue is equal to one, and the variance of the eigenvalues is zero.  When the correlation is one, the first eigenvalue is equal to six, all other eigenvalues are zero, and the variance of the eigenvalues is six. As correlations increase from .2 to .8, the size of the first eigenvalue increases, and that of the other eigenvalues decreases.  

<!--Table 1 also considers the case when the outcomes correspond to two underlying factors. Rows 7-12 show eigenvalues for the case where the first three outcomes correspond to Factor 1, and the last three outcomes correspond to Factor 2, but there is no correlation between the two factors. The correlation for outcomes loading on the same factor, is as shown in the first column. We can see that now the first two eigenvalues are equivalent in size, becoming larger as the within-factor correlation increases. Note that in this situation, the average correlation for the off-diagonal matrix will be lower than the correlation within factors; the computed mean correlation is shown in column 2. The variance of eigenvalues is higher in this case than it is for the case where all correlations are uniform - as can be seen by comparing the variance for the 2-factor case when r = 1, with the case of the 1-factor case with r = .4.-->

```{r makemat,include=F}
demoeigens<-function(nvar,cor){
m<-matrix(rep(cor,nvar*nvar),nrow=nvar)
diag(m)<-1
evs = eigen(m)$values

return(evs)
}
```

```{r makemat2,include=F}
#have correls split into two blocks
demoeigens2<-function(nvar,cor){
m<-matrix(rep(cor,nvar*nvar),nrow=nvar)
diag(m)<-1
startc1<-1
endc1<-(nvar/2)
startc2<-(nvar/2)+1
endc2<-nvar
m[startc1:endc1,startc2:endc2]<-0
m[startc2:endc2,startc1:endc1]<-0
evs = eigen(m)$values
meancor<-mean(m[upper.tri(m)])

return(list(evs,meancor))
}
```

```{r eigendemo,include=F}
#This chunk creates Table 1
nvar=6
mycor<-seq(from=0,to=1,by=.2)
evstab<-data.frame(matrix(NA,ncol=(5+nvar),nrow=(2*length(mycor)-1)))
colnames(evstab)<-c('r',paste0('Eigen',1:nvar),'Var','MEff','AlphaMEff','Mean_corr')
evstab[,1]<-c(mycor,paste0(mycor[2:length(mycor)],'/',mycor[2:length(mycor)]))
thisrow<-0
for(cor in mycor){
  thisrow<-thisrow+1
  evs<-demoeigens(nvar,cor)
  evstab[thisrow,2:(1+nvar)]<-round(evs,2)
  evstab$Var[thisrow]<-var(evs)

}

  evstab$MEff<-1+(nvar-1)*(1-evstab$Var/nvar)
  evstab$AlphaMEff<-round(.05/evstab$MEff,3)
  evstab[,2:(1+nvar)]<-numformat(evstab[,2:(1+nvar)],1)
  thistab<- evstab[1:6,1:(ncol(evstab)-1)]
ft1<-flextable(thistab)
ft1<- fit_to_width(ft1, max_width =8 )
ft1<-set_caption(ft1,'Eigenvalues, MEff and AlphaMEff with 6 outcome variables')

  #we also add to the table the case where there are 2 separate factors, but this is not included in Table 1
for(cor in mycor[2:length(mycor)]){
  thisrow<-thisrow+1
  evs<-demoeigens2(nvar,cor) #this time we have a list with evs and the computed corr
  evstab[thisrow,2:(1+nvar)]<-round(evs[[1]],2)
  evstab$Var[thisrow]<-var(evs[[1]])
  evstab$Mean_corr[thisrow]<-evs[[2]]

}



  evstab$MEff<-1+(nvar-1)*(1-evstab$Var/nvar)
  evstab$AlphaMEff<-round(.05/evstab$MEff,3)

  #move the Meancorr into 2nd column
  nc<-ncol(evstab)
  evstab<-evstab[,c(1,nc,2:(nc-1))]
  
  write.csv(evstab,'EVStab.csv',row.names=F)

  
```

`r ft1`  
In Table 1, r is the intercorrelation between the six outcomes, Eigen1-Eigen6 are the eigenvalues, and Var is the variance of the six Eigenvalues, which is used to compute MEff (the effective number of comparisons) from the formula:  
   MEff = 1 + (N-1)*(1-(Var(Eigen)/N)  
where N is the number of outcome measures, and Eigen is the set of N eigenvalues. 

This value is then used to compute the corrected alpha level, AlphaMEff. Assuming we set alpha to .05, AlphaMEff is .05 divided by MEff. One can see that this value is equivalent to the Bonferroni-corrected alpha (.05/6) when there is no correlation between variables, and equivalent to .05 when all variables are perfectly correlated. 

Derringer (2018) provided a useful tutorial on MEff, noting that it is not well-known outside the field of genetics, but is well-suited to the field of psychology. Her preprint includes links to R scripts for computing MEff and illustrates their use in three datasets. 

These resources will be sufficient for many readers interested in using MEff, but readers may find it useful to have a look-up table for the case when they are evaluating existing studies.  The goal of this paper is two-fold:  

A. To consider how inclusion of multiple outcome measures affects statistical power, relative to the case of a single outcome, when appropriate correction of the familywise error rate is made using MEff. 
B. To provide relevant information to help evaluate studies with multiple outcome measures, without requiring the reader to perform complex statistical analyses. 

These goals are achieved in three sections below:

1. Power to detect a true effect using MEff is calculated from simulated data for a range of values of sample size (N), effect size (E) and intercorrelation between outcomes (**R**)
2. A lookup table is provided that gives values of MEff, and associated adjusted alpha-levels for different set sizes of outcome measures, with mean pairwise correlation varying from 0 to 1 in steps of .1.
3. Use of the lookup table is shown for real-world example of application of MEff using a published  dataset. 

## Alternative approach, MinNVar  
In the original version of this manuscript, an alternative approach, MinNVar, was proposed, in which the focus was on the _number_ of outcome variables achieving a conventional .05 level of significance. As noted by reviewers, this has the drawback that it could not reflect continuous change in probability levels, because it was based on integer values (i.e. number of outcomes). This made it overconservative in some cases, where adopting the MinNVar approach gave a familywise error rate well below .05. One reason for proposing MinNVar was to provide a very easy approach to evaluating studies that had multiple outcomes, using a lookup table to check the number of outcomes needed, depending on overall correlation between measures. However, it is equally feasible to provide lookup tables for MEff, which is preferable on other groups, and so MinNVar is not presented here; interested readers can access the first version of this paper to evaluate that approach. 

## Use of one-tailed p-values    
In all the analyses and simulations described here, one-tailed tests are used. Two-tailed p-values are far more common in the literature, perhaps because one-tailed tests are often abused by researchers, who may switch from a two-tailed to a one-tailed p-value in order to nudge results into significance.  

This is unfortunate because, as argued by Lakens (2016), provided one has a directional hypothesis, a one-tailed test is more efficient than a two-tailed test. It is a reasonable assumption that in intervention research, which is the focus of the current paper, the hypothesis is that an outcome measure will show improvement. Of course, interventions can cause harms, but, unless those are the focus of study, we have a directional prediction for improvement. 



## Methods  
Correlated variables were simulated using in the R programming language [@rcoreteam2020]. The script to generate and analyse simulated data is available on !!!!!.  For each model specified below, 1000 simulations were run to compare two groups on a one-tailed t-test. Data for the two groups were generated by the same process, except that an effect size, E, was added to scores of the intervention group, I, but not to the control group, C. Scores of the two groups were compared using a one-tailed t-test for each run.

Power was computed for different levels of effect size (E),  correlation between outcomes (**R**) and sample size per group (N) for the following methods:  
a) Uncorrected data: Proportion of runs where p was less than .05 for at least one outcome.
b) Bonferroni-corrected data: Proportion of runs where p was less than the Bonferroni-corrected value for at least one outcome.
c) Principal component analysis (PCA): Proportion of runs where p was below .05 when groups I and C were compared on scores on the first principal component of PCA. 
d) MEff-corrected data: Proportion of runs where p was less than AlphaMeff value for at least one outcome. 
Note that where E is set to zero, the power corresponds to the false positive rate - i.e. the probability of detecting a significant result when the null  hypothesis is true.  

### Method for simulating outcomes  

Simulating multivariate data forces one to consider how to conceptualise the relationship between an intervention and multiple outcomes.  Implicit in the choice of method is an underlying causal model that includes mechanisms that lead measures to be correlated. 

## Method M (Multivariate normal) ##
Perhaps the simplest approach is to use the _mvrnorm_ function of the _MASS_ package to generate a set of outcome variables with a specified covariance matrix, where all off-diagonal elements are drawn from a population with the same prespecified correlation, r. Three parameters are varied across runs: the population correlation, r; the number of cases per group, N; and the mean difference between Intervention (I) and Control (C) groups, E (effect size). 

In initial tests, r varied from 0 to .8 in steps of .2, and the number of simulated cases per group varying from 20 to 110 in steps of 30. The mean was always zero for group C, and the specified effect size, E, for group I. Because of random noise, observed effect size in a run will vary around E; the mean of all effect sizes is expected to approximate E, though will be noisier when N is small. 

This method corresponds to that used by Vickerstaff et al. (2021), except that they also incorporated options for missing data and for non-continuous data. However, in some situations, it leads to unrealistic data: in particular it is possible to have a set of outcomes that are independent of one another (r = 0) yet all having the same effect size. In real-world data, one would expect outcomes to be correlated, especially those that all showed an impact of intervention. The implicit assumption is that the intervention is effective for several outcomes insofar as it modifies a common process. 
Conversely, if a set of outcomes was very highly intercorrelated, then we would expect them all to show a similar intervention effect. It is possible to think of counterexamples: for instance, height and weight are correlated in the adult population, yet we would expect diet to affect only weight. But in practice, nobody doing a study of a diet would use both height and weight as outcome variables: our knowledge of malleability of adult height would preclude this.  

In practice, with most combinations of parameters that were of interest, Method M gave results equivalent to the next method, Method L, but the latter was preferred as it allowed for a wider range of parameters to be used, and the underlying assumptions seemed more plausible.   

## Method L  (latent) 
With this method, the outcomes are modelled as indicators of one or more underlying latent variables, which mediate the intervention effect. This can be achieved by first simulating a latent variable, with an effect size of either zero, for group C, or E for group I. Observed outcome measures are then simulated as having a specific correlation with the latent variable - i.e. the correlation determines the extent to which the outcomes act as indicators of the latent variable. This can be achieved using the formula:  
$$
r * L + \sqrt{1-r^2} * E 
$$
where r is the correlation between latent variable (L) and each outcome, and L is a vector of random normal deviates that is the same for each outcome variable, while E (error) is a vector of random normal deviates that differs for each outcome variable. Note that when outcome variables are generated this way, the mean intercorrelation between them will be r^2^.  Thus if we want a set of outcome variables with mean intercorrelation of .4, we need to specify r in the formula above as sqrt(r) = .632. Furthermore, the effect size for the simulated variables will be lower than for the latent variable: to achieve an effect size, E, for the outcome variables, it is necessary to specify the effect size for the latent variable, E~l~, as E/r^2^. 

As noted above, the results with Method L were closely similar to those obtained using MASS, for the range of correlations and effect sizes considered here. The exception is for the case where r = 0, which is not computable with this method - i.e. it is not possible to have a set of outcomes that are indicators of the same latent factor but which are uncorrelated. As noted above, the case where r = 0 is unrealistic in any case, and so for the simulations reported here, the lowest value of r that was included was r = .2. 

Both methods M and L treat all outcome measures as equivalent. In practice, of course, we will observe different effect sizes for different outcomes, but in these models, this is purely down to the play of chance.  The simulation becomes more complex if we allow for the possibility that there may be true differences in intervention effects on different outcomes.  

## Model F: Underlying factor structure  
Rather than being indicators of the same uniform latent variable, the outcomes might correspond to different latent constructs. For instance, Vickerstaff et al (2021) described an intervention for obesity where outcomes included weight and blood glucose levels. Potentially, the intervention might affect one latent construct more than the other. Following suggestions by Sainani (2022), a set of simulations was generated to consider relative power of different methods when there are two underlying latent factors that generate the outcomes, with only one factor affected by intervention. The computational approach was the same as for model L, but with two latent variables, each used to generate a block of variables. The two latent factors are uncorrelated, and only one block of variables is has a nonzero effect of intervention. 


## Results  



## 1. Power calculations.
 
<!--Simulated data is created using makesimdata.Rmd.  
This is based on the original MinNVar approach, but it saves adjusted alphas using MEff on each run.
The filename is saved as "simulated_pMEff.csv"-->

```{r countpcritical,include=F}
countpcrit<-function(mycol,myp){
  x <-length(which(my<myp))
  return(x)
}

```

```{r powerForManyN,include=F,echo=F}
pfile<-here("data/sim_pMEff.csv")

p.df<-read.csv(pfile)
myN<- unique(p.df$Nsub)
myES<- unique(p.df$ES)
mycorr<-unique(p.df$corr)
myout<-2:12
  #Next we compute the proportion of all runs in each condition that give p < .05. This corresponds to familywise error rate when effect size is zero, and to power when effect size is > 0.
  
nouts<-c(1,myout) #we include the single case at start, and the PC (coded zero) at the end
powertab<-expand_grid(myES,myN,mycorr,nouts) #will hold % of cases with 0 to N significant outputs.
#Power will be 1-%0

addcols<-data.frame(matrix(0,nrow=nrow(powertab),ncol=(2+length(myout))))
colnames(addcols)<-paste0('sigp',c(0,1,myout))
powertab<-cbind(powertab,addcols)
  writerow <- 0
  for (e in myES){
    for (n in myN){
      for (c in mycorr){
        tempdata<-dplyr::filter(p.df,ES==e,Nsub==n,corr==c)
        
        mcol<-which(colnames(p.df)=='MEffsig2')
        for (m in c(1,mcol:ncol(p.df))){
         
          writerow<-writerow+1
      
          
          if(m==1){
            powertab[writerow,5]<-length(which(tempdata$V1>.05))/nrow(tempdata)
            powertab[writerow,6]<-1-powertab[writerow,5]
          }
          
          if(m>1){
            
            t<-table(tempdata[,m])/nrow(tempdata) #proportion with a given N significant
            nt<-as.integer(names(t))
            for(i in 1: length(nt)){
              powertab[writerow,(5+nt[i])]<-t[i]
              
            } #end of if statement
          } #end of m loop
        }
      }
    }
  }
  
  #for plotting need to reshape to long form
  
  powerlong<-reshape(powertab, direction = "long",
        varying = 5:ncol(powertab), sep = "")
  
w<-which(colnames(powerlong)=='time') #default label from reshape needs changing
colnames(powerlong)[w]<-'Nsig'
for (n in myN){
  plotbit <- filter(powerlong,myN==n)
p<-ggplot(data=plotbit, aes(x=nouts, y=sigp,fill=as.factor(Nsig))) +
  geom_bar(stat="identity")


p<-p + facet_grid(myES ~ mycorr)

plotname<-here(paste0('power_N_',n,'.jpg'))
ggsave(plotname)
}

#Now for each N, ES and corr, table with Nout by power

powerN<-expand.grid(c=mycorr,n=myN,e=myES[2:length(myES)]) #ignore ES of 0 for power
addcols<-data.frame(matrix(NA,nrow=nrow(powerN),ncol=12))
colnames(addcols)<-paste0('Nout_',1:12)
powerN<-cbind(powerN,addcols)
thisrow<-0
for (e in myES[2:length(myES)]){
 for (n in myN){
    for (c in mycorr){
      thisrow<-thisrow+1
      mm <- filter(powerlong,myES==e,myN==n,mycorr==c,Nsig==0) #cases with no sig value - ie beta which is 1-power
      powerN[thisrow,4:ncol(powerN)]<-1-mm$sigp
     
    }
  }
}

write.csv(powerN,'powerN.csv',row.names=F)
        
```



## 2. Deriving a lookup table. 

Table 2 shows corrected alpha values based on MEff, varying according to the correlation between outcome measures, and the number of outcome measures in the study. If the obtained p-value for *any* of the outcomes is below this value, this can be regarded as evidence of a positive effect of the intervention.  



```{r makeMEff,include=F}
makeMEff<-function(allN,allr,method=1){
#the default method is 1, which is when all off-diagonal correlations are equal

MEfftab<-data.frame(matrix(NA,nrow=length(allr),ncol=1+length(allN))) #data frame to hold results
colnames(MEfftab)<-c('corr',paste0('N',allN))
MEfftab[,1]<-allr
alphatab<-MEfftab #copy the output table to make another table for alpha values
comprtab<-MEfftab #used to hold computed off diagonal means - used to check outputs for methods 2-3
myrow<-0
mycol<-0
for (i in allN){
  mycol<-mycol+1
  myrow<-0
  for (j in allr){
    myrow<-myrow+1
    
    mymat<-matrix(j,nrow=i,ncol=i)#correlation matrix with all values at j
    diag(mymat) <- 1
    
if(method==2){
#if method is 2, then offdiagonal values across sets are zero
# we assume we have even number for N, and make 2 sets
      set1<-1:round(i/2,0)
      set2<-(1+max(set1)):i

      for(k in set1){ #across the sets the correlation is zero
        for (l in set2){
          mymat[k,l]<-0
          mymat[l,k]<-0
        }
      }
}

n<-dim(mymat)[1]
uppert<-mymat[upper.tri(mymat)]
#compmean<-mean(FisherZ(uppert)) #computed mean for upper triangle
compmean<-mean(uppert)



    evs = eigen(mymat)$values
    eff = 1 + (i - 1) * (1 - var(evs) / i)
    MEfftab[myrow,(1+mycol)]<-round(eff,3)
    comprtab[myrow,(1+mycol)]<-round(compmean,3)
 }
}
alphatab<-MEfftab
alphatab[,2:ncol(alphatab)]<-round(.05/alphatab[,2:ncol(alphatab)],3)
return(list(MEfftab,alphatab,comprtab))
}
```

```{r runmakeMEffv1,include=F}
allN<-2:12
allr<-seq(0,1,.1)

myout<-makeMEff(allN,allr,method=1)
MEfftab1<-myout[[1]]
alphatab1<-myout[[2]]
ft2<-flextable(alphatab1)
ft2<- fit_to_width(ft2, max_width =8 )
ft2<-set_caption(ft2,'AlphaMEff for different correlation values with 12 outcome variables')

```

`r ft2`  
     
## Methods



```{r MethodLobsES,echo=F}
#check how obsES relates to trueES -expect to be same for Method M, lower for L
# [The ES and observed ES fall on a straight line  for Method M].
# This chunk is superseded - it was used when evaluating the latent variable method to check how effect sizes on outcome variables compared with the specified effect size. 
runme <- 0
if(runme ==1){
  powertab<-read.csv('data/powertab_methodL.csv')
  method='L'
  fignum<-fignum+1
  plot(powertab$ES,powertab$obsES,col=as.factor(powertab$corr),xlab='True ES in latent factor',ylab='Observed ES',main=paste0('Figure ',fignum,': Observed effect sizes with Method ',method))
  abline(a=0,b=1,lty=2)
  legend(.05,.6, legend=c('0','.2','.4','.6','.8'),title='correlation',
         col=1:5, lty=1, cex=0.8)
}

```

### Data reduction    
The size of the suite of outcome variables entered into later analysis ranged from 2 to 12. For each suite size, a corrected alpha value was computed from the MEff value obtained the simulated correlation matix. 


### Simulation parameters
10,000 simulations were run for each combination of:  
- sample size per group, ranging from 20 to 110 in steps of 30  
- correlation between outcome variables, ranging from .2 to .8 in steps of .2  
- true effect size, taking values of 0, .3, .5, or .7.  

The data generated from each combination of conditions was used to derive results for different sizes of suites of outcome variables, ranging from 2 to 12. Thus, the analysis was first conducted on the first 2 outcome measures, then on the first 3 outcome measures, and so on. 

For each set of conditions, on each run, a one-tailed t-test was conducted to obtain a p-value for the comparison between C and I groups, assuming C would be lower. The p-values for outcome measures were rank ordered for each run and each suite size. 



### Computing power using MEff
For each run of the simulation, and each number of outcome measures, we take the value of MEff from the previous step and compute the proportion of p-values below .05, depending on the effect size, sample size and correlation between measures. For effect sizes above zero, this proportion corresponds to the statistical power. 

Power using Adjust NVar can be compared to:  
- power obtained with a single outcome measure for the same effect size and sample size  
- power obtained by using the principal component extracted for this set of outcome measures  


## Results



The Principal Components plots show all points are to the right of the vertical line denoting power from a single outcome, i.e. this method achieves higher power than a single outcome measure for each size of suite of outcomes. Familywise error rate clusters around .05. If we compare how Adjust NVar compares with Principal Components, with 3 or more outcomes the power is generally slightly lower, but the tradeoff between power and familywise error (expressed as a ratio) is higher for Adjust NVar. 

## Discussion

Some interventions are expected to affect a range of related processes.  In such cases, the need to specify a single primary outcome tends to create difficulties, because it is often unclear which of a suite of outcomes is likely to show an effect. Note that the MEff approach does not give the researcher free rein to engage in p-hacking: the larger the suite of measures included in the study, the lower the adjusted alpha will be. It does, however, remove the need to put all one's eggs in one basket by pre-specifying one measure as the primary outcome. 

A second advantage is that in effect, by including multiple outcome measures, one can improve the efficiency of a study, in terms of the trade-off between power and familywise errors. A set of outcome measures may be regarded as imperfect proxy indicators of an underlying latent construct, so we are in effect building in a degree of within-study replication if we require that more than one measure shows the same effect in the same direction before we reject the null hypothesis. 

The comparison with power and familywise error rate from principal components shows that ...

A possible disadvantage of using the MEff over principal components is that this approach is likely to tempt researchers to interpret specific outcomes that fall below the .05 threshold as meaningful. They may be, of course, but this simulation demonstrates that when we create a suite of outcomes that differ only by chance, it is common for only a subset of them to reach the significance criterion. Any recommendation to use MEff should be accompanied by a warning that a suite of outcomes should be selected as representative of the underlying construct the intervention is designed to influence, in effect serving as replicate measures, all of which should be equally promising as indicators of an intervention effect. If a subset of outcomes show an effect of intervention, this could be due to chance. It would be necessary to run a replication to have confidence in a particular pattern of results. 

It is also worth noting that results obtained with this approach depend crucially on assumptions embodied in the simulation that is used to derive predictions. Outcome measures simulated here are normally distributed, and uniform in their covariance structure.  It would be of interest to evaluate MEff in datasets with different variable types, such as those used by Vickerstaff et al (2021) that included binary as well as continuous data, as well as modeling the impact of missing data. 


Compared to most other approaches, MEff is relatively simple. It could potentially be used to reevaluate published studies that report multiple outcomes but may not have been analysed optimally, provided we have some information on the average correlation between outcome measures. 

In sum, the veto on use of multiple outcomes in intervention studies does not lead to optimal study design. Inclusion of several related outcomes can increase statistical power, without increasing the false positive rate, provided appropriate correction is made for the multiple testing. 



# Appendix

Computed power for Adjust NVar method
```{r getpowertabsM,echo=F}
powerM<-read.csv('data/powertab_methodM.csv')

powerM$obsES<-round(powerM$obsES,3)
pcstart <- which(colnames(powerM)=='PC2')
powerMA <- powerM[,1:(pcstart-1)]
ft<-flextable(powerMA)
ft<- fit_to_width(ft, max_width =8 )
ft<-set_caption(ft,'Power table for Adjusted NVar Method')
ft

powerMB <- powerM[,c(1:4,pcstart:ncol(powerM))]
ft2<-flextable(powerMB)
ft2<- fit_to_width(ft2, max_width =8 )
ft2<-set_caption(ft2,'Power table for principal components')
ft2
```

# Table titles  


# Figure titles  



# Notes  

The script is available on https://github.com/oscci/MinSigVar.

# Competing interests
No competing interests were disclosed.  

# References


