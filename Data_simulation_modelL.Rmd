

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(flextable)
require(MASS)
require(pwr)
require(graphics)
require(here)
require(reshape2) #for heatmap
```


```{r makemodelL,echo=F}
# Minor updates on 28 Oct 2023 to remove lines for effect size = .8 from figures
# Version saved on OSF 17 Nov 2022
# Only subsequent change is to make .tiff versions of figures and label fig columns as r:
# # model L
# This function generates a mylatent vector plus maxn variables for N subjects.
# The mylatent vector is saved in the last column. This is not returned as it is only used to check that the correct correlation structure has been achieved.
# The maxn variables are saved in the first maxn columns. They have a correlation, r, with the mylatent vector.
# The average intercorrelation between maxn variables is sqrt(r)
# The columns after maxn contain the principal component scores from the different levels of N outcomes for each subject, e.g. PCA2, PCA4, and PCA6, if we are considering outcome sets of 2, 4 or 6 variables. 
makecorL <- function(N,outcomes,r,E,sd){ #we generate raw data for max value of outcomes
  #principal components created at a later step
  #  NB outcomes is a vector of possible N outcomes - e.g. c(2, 4, 6) if you want to simulate data to consider 2, 4, or 6 outcomes
  # we always simulate individual variables up to the maximum number of outcomes, e.g. in examples above, would simulate 6 outcome variables. These are in the first columns.
  
  maxn <- max(outcomes)
   rvals <-data.frame(matrix(NA,nrow=N,ncol=(1+maxn+length(outcomes)))) 
  colnames(rvals)<-c(paste0("V",1:maxn),paste0("PC",outcomes),'latent')
  ncols <- ncol(rvals)
  mylatent <- rnorm(N,E,sd) #this is the latent variable
  for (v in 1:maxn){
    rvals[,v]<-r*mylatent+sqrt(1-r^2)*rnorm(N,0,1) #r determines correlation with latent variable, mylatent
  }
  
  return(rvals) 
}
```

```{r makemodelL2,echo=F}
# # Same as model L, except 2 values of mylatent are simulated, one for each block of variables
#if model is specified as 2, then 2 blocks both with effect. If model specified as 3, then 2 blocks, only one of which has effect
makecorL2 <- function(N,outcomes,r,E,sd,mymodel){ #we generate raw data for max value of outcomes
  
  maxn <- max(outcomes)
   rvals <-data.frame(matrix(NA,nrow=N,ncol=(1+maxn+length(outcomes)))) 
  colnames(rvals)<-c(paste0("V",1:maxn),paste0("PC",outcomes),'latent')
  ncols <- ncol(rvals)
  mylatent <- rnorm(N,E,sd) #this is the latent variable for block 1 (odd outcomes)
  mylatent2 <- rnorm(N,E,sd) #this is the latent variable2 for 2nd block (even outcomes)
  if(mymodel==3)
  {
      mylatent2 <- rnorm(N,0,sd)  #for model 2 the 2nd block always has null effect
  }
  #if we want case where only factor 1 has effect, then use 0 rather than E for mylatent2
   block1 <- seq(1,maxn,2)
   block2 <- seq(2,maxn,2)
  for (v in block1){
    rvals[,v]<-r*mylatent+sqrt(1-r^2)*rnorm(N,0,1) #r determines correlation with latent variable, mylatent
  }
   for (v in block2){
    rvals[,v]<-r*mylatent2+sqrt(1-r^2)*rnorm(N,0,1) #r determines correlation with latent variable, mylatent
  }
  
  return(rvals) #we don't save the mylatent value, which is latent
}
```



```{r createp,echo=F}
# Make data frame of p-values.
#For each run of the simulation, a one-tailed t-test is used to compare groups I and C; we can use a one-tailed test as we assume the intervention group (I) will obtain a higher score than the control group (C).  This generates a huge data frame of p-values, one per run, covering all values of effect size, correlation and sample size. P-values are given for individual variables, and for the principal components. 
#Here we use model L1 
#model L1: all outcomes load on the same latent factor
#model L2: outcomes divided by two latent factors but both show effect
#model L3: (This is model L2x in the paper) outcomes divided by two latent factors, only one shows effect.
  
model <- "L1"
alpha <- .05
nsim <- 50 #number of runs for each combination of mycorr, mynvar,myES and myN.
#for final version, have v. large value - for testing try with smaller nsims
mycorr <- c(.2,.4,.6,.8) #intercorrelations between variables to consider
nstep<-2 
outcomes <- seq(2,12,nstep) #number of outcome variables to consider
maxn <- max(outcomes) #we always create max number of variables, and then use subsets of this to examine smaller Ns
myES <- c(0,.3,.5,.8) #effect sizes to consider
myN <- c(20,50,80) #Ns per group to consider
mypname <-paste0('p_1side_model',model,'_allN_allES_allcorr_maxn',maxn,'_nsim',nsim,'_nstep',nstep) #filename to save csv file with simulated p-values (saves time when many runs used)
myfolder <-'data/'
  #set up dataframe for holding pvalues
  p.df <- data.frame(matrix(NA,nrow=nsim*length(mycorr)*length(myN)*length(myES),ncol=maxn+6+2*length(outcomes)))
  colnames(p.df) <-c('run','ES','Nsub','corr','meanr','obsES',paste0('V',1:maxn),paste0('PC',outcomes),paste0('alphaMEff',outcomes)) 
  coloffset <- which(colnames(p.df)=='V1')-1
  thisrow <- 0
  for (s in 1:nsim){
    if(s/20==round(s/20,0)){print(s)}
    for (rval in 1:length(mycorr)){
      for (e in myES){
        for(n in myN){
          thisrow <- thisrow+1
          c<-mycorr[rval]
          p.df$run[thisrow] <- s
          p.df$corr[thisrow] <- c
          p.df$ES[thisrow]<-e
          p.df$Nsub[thisrow]<-n
          
#makecorL and makecorL2 create a data frame for each run with scores for outcomes
#Those in dataframe I have the effect size added to the latent variable
          
if (model=='L1'){
            C <- makecorL(n,outcomes,sqrt(c),0,1) #intercorrelations will be mycor^2
            I <- makecorL(n,outcomes,sqrt(c),e/sqrt(c),1)  #group I is intervention group: has ES added to mylatent value. Note that to get correct E for the observed vars, need to use e/sqrt(u)
            
}
if(model=='L2'){
  mymodel<-2
  #note that the function now has a final 'mymodel' term that distinguishes L2 and L3
            C <- makecorL2(n,outcomes,sqrt(c),0,1,mymodel) #intercorrelations will be mycor^2
            I <- makecorL2(n,outcomes,sqrt(c),e/sqrt(c),1,mymodel) #group I is intervention group: has ES added to mylatent value
            #for model L2, same as L1, except outcomes load on 2 factors corresponding to odd or even measures 
        
}    
          if(model=='L3'){ #in the write-up this is termed model L2x
            mymodel <- 3
  #note that the function now has a final 'mymodel' term that distinguishes L2 and L3
            C <- makecorL2(n,outcomes,sqrt(c),0,1,mymodel) #intercorrelations will be mycor^2
            I <- makecorL2(n,outcomes,sqrt(c),e/sqrt(c),1,mymodel) #group I is intervention group: has ES added to mylatent value
            #for model L2, same as L1, except outcomes load on 2 factors corresponding to odd or even measures 
        
}   
        #create principal components: this based on I and C vals combined
        thiscol<-maxn #we will add PCs as next set of cols
        n2 <- 2*nrow(C)
        for(o in outcomes){
          
          varboth <-rbind(I[,1:o],C[,1:o]) #both groups, all columns for this outcome suite
          pwts<- prcomp(varboth)$rotation[,1]
      #principal component may have opposite polarity, so flip if wts negative
          if(mean(pwts)<0){pwts<-pwts*(-1)}
           
          mypc<-apply(varboth[,1:o],1,function(x) sum(pwts*x)) #compute principal component score for each row of data
          thiscol<-thiscol+1
          I[,thiscol]<-mypc[1:nrow(I)]
          C[,thiscol]<-mypc[(1+nrow(I)):n2]
          
                      
       #compute alphaMeff from eigenvalues from correlation matrix
          mymat<-cor(varboth)
          evs <- eigen(mymat)$values
          MEff <- 1 + (o - 1) * (1 - var(evs) / o)
          alphaMEff <- alpha/MEff
          mycolname<-paste0('alphaMEff',o)
          w<-which(colnames(p.df)==mycolname)
          p.df[thisrow,w]<-alphaMEff
        }
        
        #get pvalue from t-test for all variables and PCs
          for (v in 1:(maxn+length(outcomes))) {
            tempt <- t.test(C[,v],I[,v],alternative='less') #one-tailed t-test ; predict C<I
            p.df[thisrow,(v+coloffset)]<-tempt$p.value
            
          }
       
 
        #compute average observed effect size
          allC<-pull(C[,1:maxn]) #converts all df values to vector
          allI<-pull(I[,1:maxn])
          p.df$obsES[thisrow]<-(mean(allI)-mean(allC))/mean(sd(c(allC),sd(allI)))
          
        #compute averaged off-diagonal value of r 
        mymat<-cor(varboth)
        meanr<-mean(mymat[upper.tri(mymat)])
        p.df$meanr[thisrow]<-meanr
        
  
        }
      }
    }
  }
  write.csv(p.df,paste0(myfolder,mypname,'.csv'),row.names=F)
  # we now have p.df, which is a big dataframe with p-values from all the simulations, for max N variables at all corrs, effect sizes and sample sizes.
```

NB To avoid inadvertently overwriting file when testing script, we manually rename the saved p.df before running the next chunk.
Simplified name is just simulated_PMEff_L.csv', with model type (1, 2 or 3) after L


```{r countsigp}
#identify the file for the specific model (1, 2 or 3)
#This chunk is manually run for each model type; user specifies model at the top
model <-3
alpha<-.05

nupname<-paste0('data/simulated_pMEff_L',model,'.csv')
pfile<-here(nupname)
p.df<-read.csv(pfile)

myN<- unique(p.df$Nsub)
myES<- unique(p.df$ES)
mycorr<-unique(p.df$corr)
if(mycorr[1]==0) {mycorr<-mycorr[-1]} #
nouts<-seq(2,12,2) #N output variables, assuming step size 2
nouts<-c(1,nouts) #include case of single variable

  #We compute the proportion of all runs in each condition that give p < .05. This corresponds to familywise error rate when effect size is zero, and to power when effect size is > 0.
  
powertab<-expand_grid(myES,myN,mycorr,nouts) #will hold % of cases with 0 to N significant outputs.
#Power will be 1-%0

powertab$meancorr<-NA  #mean correlation off diagonal - for model 1 should be close to corr
powertab$Bonf.sigp<-NA #N runs signif with Bonferroni corrected p
powertab$PCA.sigp<-NA # N runs signif with PCA
powertab$MEff.sigp<-NA #N runs signif with MEff


  writerow <- 0
   for (e in myES){
        for (n in myN){
       for (c in mycorr){
   
        tempdata<-dplyr::filter(p.df,ES==e,Nsub==n,corr==c)
        nsim<-nrow(tempdata)
        mymeancorr<-round(mean(tempdata$meanr),3) #mean offdiagonal correlation (regardless of n outcomes)
 
         for (o in nouts){
          #find columns relevant for this suite size
        vcol<-which(colnames(p.df)=='V1')
        vrange<-vcol:(vcol+o-1)
        pcol<-which(colnames(p.df)==paste0('PC',o))
        mcol<-which(colnames(p.df)==paste0('alphaMEff',o))
        writerow<-writerow+1
        
    #First do Bonferroni; critical alpha depends on nouts  
        
        crit.alpha<-alpha/o
        t<-as.data.frame(tempdata[,vrange]) #need to specify data frame or problems ensure from the case where o = 1
        t1<-t
        t1[,]<- 0
        for (mycol in 1:ncol(t1)){
        w<-which(t[,mycol]<crit.alpha)
        t1[w,mycol]<-1
        }
        pvector<-t1[,1] #default for one outcome
        if(o>1){
        pvector<-rowSums(t1[,1:o])
        }
        powertab$Bonf.sigp[writerow]<-length(which(pvector>0))
      
      #Next do PCA
        if(o>1){
       crit.alpha=.05
       powertab$PCA.sigp[writerow]<-length(which(tempdata[,pcol]<crit.alpha))
        }
        
      #Next do MEff
        if(o>1){
        t<-tempdata[,c(vrange,mcol)]
        t1<-t
        t1[,1:o]<- 0
        for (mycol in 1:ncol(t1)){
        w<-which(t[,mycol]<t[,(o+1)]) #find values less than MEff for this suite size
        t1[w,mycol]<-1
        }
        pvector<-rowSums(t1[,1:o])
        powertab$MEff.sigp[writerow]<-length(which(pvector>0))
        }
        powertab$meancorr[writerow]<-mymeancorr
        
          } #end of m loop

        }
      }
    }
  

  

```
  
Powertabs are quick to create, so no need to save, though I do have saved versions in data.

```{r dopowerplot}  
#for plotting need to reshape to long form
  
#focus on nouts from 2 to 8 and ditch null effects
newnouts<-c(1,2,4,6,8)
powertab<-filter(powertab,nouts%in% newnouts,myES>0)
w<-which(colnames(powertab)=='mycorr')
colnames(powertab)[w]<-'r' #change name for plot
myN<-c(20,50,80)
for (n in myN){
power3 <- round(pwr.t.test(n=n,d=0.3,sig.level=.05,alternative="greater")$power,3)
power5 <- round(pwr.t.test(n=n,d=0.5,sig.level=.05,alternative="greater")$power,3)
power8 <- round(pwr.t.test(n=n,d=0.8,sig.level=.05,alternative="greater")$power,3)
if(model==3){power3<- .5*(power3+.05)}
if(model==3){power5<- .5*(power5+.05)}
if(model==3){power8<- .5*(power8+.05)}

  plotbit <- filter(powertab,myN==n)
  #make long form with the 3 models stacked above each other
  plotbitlong1<-plotbit[,1:6]
  colnames(plotbitlong1)[6]<-'sigp'
  plotbitlong1$method<-'Bonferroni'
  
  plotbitlong2<-plotbit[,c(1:5,7)]
    colnames(plotbitlong2)[6]<-'sigp'
    plotbitlong2$method<-'PCA'
    plotbitlong3<-plotbit[,c(1:5,8)]
      colnames(plotbitlong3)[6]<-'sigp'
  plotbitlong3$method<-'MEff'
  plotbitlong<-rbind(plotbitlong1,plotbitlong2,plotbitlong3)

  plotbitlong$sigp<-plotbitlong$sigp/nsim

  plotbitlong$Correction = as.factor(plotbitlong$method)
  plotbitlong$`Effect size` = as.factor(plotbitlong$myES)
  plotbitlong<-filter(plotbitlong,nouts>1)
  #Added in response to rev 1 - remove effect size .8
   plotbitlong<-filter(plotbitlong,myES<.8)
  p<-ggplot(plotbitlong,aes(x=nouts,y=sigp))+
    xlim(0,8)+
  geom_line(aes(color=Correction,linetype=`Effect size`))+
  geom_point(x=1,y=power3,shape=0)+
  geom_point(x=1,y=power5,shape=1)+
  #geom_point(x=1,y=power8,shape=2)+
  labs(x='N outcomes',y = 'Power')
  
  

p<-p + facet_grid( ~ r,labeller=label_both) #all label to mycorr



plotname<-here(paste0('Figures/New_model_',model,'_power_N_',n,'.tiff'))
ggsave(plotname,width=8,height=4)

}

```


