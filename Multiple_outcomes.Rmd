---
title: 'Using multiple outcomes in intervention studies to increase statistical power:
  the Adjust NVar approach'
author: "D. V. M. Bishop"
date: "23/08/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
bibliography: refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(flextable)
require(MASS)
require(pwr)
require(graphics)
require(effsize)
fignum <- 0
tabnum <-0

```


## Abstract

The CONSORT guidelines for clinical trials recommend that the researcher should specify a single primary outcome, to guard against the raised risk of false positive findings when multiple measures are considered. It is, however, possible to include a suite of multiple outcomes in an intervention study, while controlling the familywise error rate, provided the criterion for rejecting the null hypothesis specifies that N or more of the outcomes reach an agreed level of statistical significance, where N depends on the total number of outcome measures included in the study, and the correlation between them. I present simulations that explore the case when between 2 and 12 outcome measures are included in a suite, with the average correlation between measures ranging from zero to .8, and the true effect size ranging from 0 to .7. Two different methods of simulating outcome measures are compared, using a conventional null-hypothesis significance testing approach with alpha set at .05. In step 1, a table is created giving the minimum N significant outcomes (MinNSig) that is required for a given size of suite of outcome measures to control the familywise error rate at 5%. These values are used in the Adjust Nvar approach adopted in step 2. For this step, data are simulated in which the MinNSig values are used for each size of suite of correlated outcomes and the resulting proportion of significant results is computed at different sample sizes and effect sizes. The Adjust Nvar approach can achieve a more efficient ratio between power and familywise error rate than use of a single outcome when the suite includes 6 or more moderately intercorrelated outcome variables, and is considerably better in this regard than data reduction by extraction of a single principal component. Where it is feasible to have a suite of correlated outcome measures, then this might be a more efficient approach than reliance on a single primary outcome measure. In effect, it builds in an internal replication to the study. 

## The case against multiple outcomes

The CONSORT guidelines for clinical trials [@moher2010] are very clear on the importance of having a single primary outcome:  
_All RCTs assess response variables, or outcomes (end points), for which the groups are compared. Most trials have several outcomes, some of which are of more interest than others. The primary outcome measure is the pre-specified outcome considered to be of greatest importance to relevant stakeholders (such a patients, policy makers, clinicians, funders) and is usually the one used in the sample size calculation. Some trials may have more than one primary outcome. Having several primary outcomes, however, incurs the problems of interpretation associated with multiplicity of analyses and is not recommended._  

This advice often creates a dilemma for the researcher: in many situations there are multiple measures that could plausibly be used to index the outcome. A common solution is to apply a Bonferroni correction to the alpha level used to test significance of individual measures, but this is over-conservative if, as is usually the case, the different outcomes are intercorrelated. Alternative methods are to adopt some process of data reduction, such as extracting a principal component from the measures that can be used as the primary outcome, or using a permutation test to derive exact probability of an observed pattern of results. Here I explore a further, very simple, option which I term the "Adjust Nvar" approach. The idea is that if one has a suite of  outcomes, instead of adjusting the alpha level, one can adjust the number of outcomes that are required to achieve significance at the conventional alpha level of .05 to maintain an overall familywise error rate of 1 in 20 or less.  

To illustrate the idea with a realistic example, consider a behavioural intervention that is designed to improve language and literacy, and there are 6 measures where we might plausibly expect to see some benefit. The average intercorrelation between measures is .4. Suppose we find that none of the outcomes achieves Bonferroni-adjusted significance criterion of p < .008, but two of them reach significance at p < .05. Should we dismiss the trial as showing no benefit?  We can use the binomial theorem to check the probability of obtaining this result if the null hypothesis is true and the measures are independent: it is `r round(binom.test(2,6,.05)$p.value,3)`, clearly below the 5% alpha level. But what if the measures are intercorrelated? A thought experiment helps here. Suppose we had six measures that were intercorrelated at .95 - in effect they would all be measures of the same thing, and so we would expect the probability of a false positive to approximate that obtained for a single measure. Extending this logic in a more graded way, the higher the correlation between the measures, the more measures would need to reach the original significance criterion to maintain the overall significance level below .05. 

A simulation script was developed to test these intuitions and to obtain estimates of:  
-(A) the minimum number of outcome variables in a suite that would maintain the overall familywise error rate at 1 in 20, if each individual measure was evaluated at the significance criterion of .05. This we term MinNSig.  
-(B) the power to detect a true effect, if the criterion for rejection the null hypothesis was based on the value of MinNSig identified at step A.  

<!--- main text follows computation script, as we need to refer to some of the outputs-->


<!---  FUNCTIONS ----->

```{r makeMethodL,echo=F}
# # METHOD L
# This function generates a base vector plus maxn variables for N subjects.
# The base vector is saved in the last column. This is not returned as it is only used to check that the correct correlation structure has been achieved.
# The maxn variables are saved in the first maxn columns. They have a correlation, r, with the base vector.
# The average intercorrelation between maxn variables is sqrt(r)
# The columns after maxn contain the principal component scores from the different levels of N outcomes for each subject, e.g. PCA2, PCA4, and PCA6, if we are considering outcome sets of 2, 4 or 6 variables. 

makecorL <- function(N,outcomes,r,e,sd){ #we generate raw data for max value of outcomes, and then create principal components for all N outcome values
  #  NB outcomes is a vector of possible N outcomes - e.g. c(2, 4, 6) if you want to simulate data to consider 2, 4, or 6 outcomes
  # we always simulate individual variables up to the maximum number of outcomes, e.g. in examples above, would simulate 6 outcome variables. These are in the first columns.
  # We also simulate a principal component (PC) for each outcome set, i.e. a PCA based just on variables 1-2, and another based on variables 1-4, and another on variables 1-6. 
  
  maxn <- max(outcomes)
  C <-data.frame(matrix(NA,nrow=N,ncol=(1+maxn+length(outcomes))))
  colnames(C)<-c(paste0("V",1:maxn),paste0("PC",outcomes),'base')
  I <-C #same structure of data frame for intervention data
  ncols <- ncol(rvalsI)
  baseC <- rnorm(N,0,sd)
  baseI <- rnorm(N,e,sd)
  for (v in 1:maxn){
    C[,v]<-r*baseC+sqrt(1-r^2)*rnorm(N,0,1)
    I[,v]<-r*baseI+sqrt(1-r^2)*rnorm(N,0,1)
  }
  C[,ncols]<-baseC #can be used for checking correlations - otherwise not used
  I[,ncols]<-baseI
  cond <- c(rep("C",N),rep("I",N))
  allvals <-cbind(rbind(C,I),cond)
  
  #create principal components: this based on I and C vals combined
    thiscol<-maxn #we will add PCs as next set of cols
     n2 <- 2*nrow(C)
        for(o in outcomes){
          
          varboth <-allvals[,1:o] #both groups, N columns for this outcome suite

          pwts<- prcomp(varboth)$rotation[,1]
      #principal component may have opposite polarity, so flip if wts negative
          if(mean(pwts)<0){pwts<-pwts*(-1)}
           
          mypc<-apply(varboth[,1:o],1,function(x) sum(pwts*x)) #compute principal component score for each row of data
          thiscol<-thiscol+1
          allvals[,thiscol]<-mypc
         
        }
        
          #compute average observed effect size
        dd<-0
        for (d in 1:maxn){
          dd<-dd+cohen.d(allvals[,d]~allvals$cond)$estimate
        }
        allout<-(-dd/maxn)  #first value of allout will be average effect size from all 12 simulated vars in this condition
    
          
        #get pvalue from t-test for all variables and PCs
          for (v in 1:(maxn+length(outcomes))) {
            tempt <- t.test(allvals[,v]~allvals$cond,alternative='less') #one-tailed t-test ; predict C<I
            allout<-c(allout,tempt$p.value)
            
          }


  #cor(rvals) #checking correlations

  #NB principal component correlates higher with individual vars than the base component - 
  #presumably because PC is derived from the data.
  return(allout) #we don't save the base value, which is latent
}

```

```{r makecorrelated_MASS,echo=F}
# METHOD M
# This function generates a base vector plus maxn variables for N subjects.
# The base vector is saved in the last column. This is not returned as it is only used to check that the correct correlation structure has been achieved.
# The maxn variables are saved in the first maxn columns. They have a correlation, r, with the base vector.
# The average intercorrelation between maxn variables is sqrt(r)
# The columns after maxn contain the principal component scores from the different levels of N outcomes for each subject, e.g. PCA2, PCA4, and PCA6, if we are considering outcome sets of 2, 4 or 6 variables. 

makecorM <- function(N,outcomes,r,e,sd){ 
  #Alternative approach: just generate correlated vars with mu set to ES 
  # We also simulate a principal component (PC) for each outcome set, i.e. a PCA based just on variables 1-2, and another based on variables 1-4, and another on variables 1-6. 
  
  maxn <- max(outcomes)
  C <-data.frame(matrix(NA,nrow=N,ncol=(1+maxn+length(outcomes))))
  colnames(C)<-c(paste0("V",1:maxn),paste0("PC",outcomes),'base')
  I <-C #same structure of data frame for intervention data
  ncols <- ncol(rvalsI)
  
  cormat <-matrix(r,nrow=maxn,ncol=maxn)
  diag(cormat) <- 1
  C[,1:maxn] <- mvrnorm(N,rep(0,maxn),cormat)
  I[,1:maxn] <- mvrnorm(N,rep(e,maxn),cormat)
  cond <- c(rep("C",N),rep("I",N))
  allvals <-cbind(rbind(C,I),cond)
  
 #create principal components: this based on I and C vals combined
    thiscol<-maxn #we will add PCs as next set of cols
        for(o in outcomes){
          
          varboth <-allvals[,1:o] #both groups, N columns for this outcome suite

          pwts<- prcomp(varboth)$rotation[,1]
      #principal component may have opposite polarity, so flip if wts negative
          if(mean(pwts)<0){pwts<-pwts*(-1)}
           
          mypc<-apply(varboth[,1:o],1,function(x) sum(pwts*x)) #compute principal component score for each row of data
          thiscol<-thiscol+1
          allvals[,thiscol]<-mypc
         
        }
        
          #compute average observed effect size
        dd<-0
        for (d in 1:maxn){
          dd<-dd+cohen.d(allvals[,d]~allvals$cond)$estimate
        }
        allout<-(-dd/maxn)  #first value of allout will be average effect size from all 12 simulated vars in this condition
    
          
        #get pvalue from t-test for all variables and PCs
          for (v in 1:(maxn+length(outcomes))) {
            tempt <- t.test(allvals[,v]~allvals$cond,alternative='less') #one-tailed t-test ; predict C<I
            allout<-c(allout,tempt$p.value)
            
          }


return(allout)  
}

```

```{r rankedp, echo=F,include=F}
rankp <- function(p.df,outcomes){
  coloffset <- which(colnames(p.df)=='V1')-1
  for (o in outcomes){
    #first create o blank columns to hold ranked values
    lastcol<-ncol(p.df)
    p.df[,(lastcol+1):(lastcol+o)]<-NA
    for (x in 1:o){
      colnames( p.df)[(x+lastcol)] <- paste0('r',o,'.',x)
    }
    
    p.df[,(lastcol+1):(lastcol+o)]<-t(apply(p.df[,6:(coloffset+o)],1,function(x) x[order(x)]))
  }
  #write.csv(p.df,paste0(myfolder,mypname,'.csv'),row.names=F) - can save file with the added ranks, but this can create duplicates if already saved and read in.
  return(p.df)
}
```


```{r createp,echo=F}
# For each run of the simulation, a one-tailed t-test is used to compare groups I and C; we can use a one-tailed test as we assume the intervention group (I) will obtain a higher score than the control group (C).  This generates a huge data frame of p-values, one per run, covering all values of effect size, correlation and sample size. P-values are given for individual variables, and for the principal components. 



skipsim <- 1 #we can skip the simulation process and read data from previous run if we set skipsim to one 
#methodlist <-c("L","M")#simulation method L, use latent factor; method M, use MASS
methodlist<-"M"
for (m in 1:length(methodlist)){
  
method <- methodlist[m] 

# If skipsim is 0 and nsim is 1000, this takes about 50 min to run on my mac

nsim <- 1000 #number of runs for each combination of mycorr, mynvar,myES and myN.
#for final version, have v. large value - for testing try with smaller nsims

mycorr <- c(0,.2,.4,.6,.8) #intercorrelations between variables to consider

#usecorr <- c(.0001,.2,.4,.6,.8)#we use lower val of .0001 as equivalent to null as this makes it possible to get correct ES when we use method L
nstep<-1
outcomes <- seq(2,12,nstep) #number of outcome variables to consider
maxn <- max(outcomes) #we always create max number of variables, and then use subsets of this to examine smaller Ns
myES <- c(0,.3,.5,.7) #effect sizes to consider
myN <- c(20,50,80,110) #Ns per group to consider

mypname <-paste0('p_1sided_method',method,'_allN_allES_allcorr_maxn',maxn,'_nsim',nsim,'_nstep',nstep) #filename to save csv file with simulated p-values (saves time when many runs used)
myfolder <-'data/'


if(skipsim==0){ 
  #set up dataframe for holding pvalues
 
  
  p.df <- data.frame(matrix(NA,nrow=nsim*length(mycorr)*length(myN)*length(myES),ncol=maxn+5+length(outcomes)))
   p.df[1:4] <- expand.grid(1:nsim,myES,myN,mycorr)
  colnames(p.df) <-c('run','ES','Nsub','corr','obsES',paste0('V',1:maxn),paste0('PC',outcomes) ) 
  p.df$run <- 1:nrow(p.df)
  coloffset <- which(colnames(p.df)=='V1')-1
  thisrow <- 0
  
 # !loop seems unavoidable!
 for (c in 1:length(mycorr) ){
      u <-usecorr[c] #value with .0001 instead of zero
      r <- mycorr[c]
       for (e in myES){
         for (n in myN){
    
       myf<-filter(p.df,ES==e,Nsub==n,corr==r)
       if(method=="M"){
       p.df[myf$run,5:(ncol(p.df))]<-t(apply(myf,1, function(x) makecorM(n,outcomes,r,e,1)) )
       }
       if(method=="L"){
       p.df[myf$run,5:(ncol(p.df))]<-t(apply(myf,1, function(x) makecorL(n,outcomes,r,e,1)) )
       }
     }
   }
 }

          
  write.csv(p.df,paste0(myfolder,mypname,'.csv'),row.names=F)
  # we now have p.df, which is a big dataframe with p-values from all the simulations, for max N variables at all corrs, effect sizes and sample sizes.
}

}
```



```{r nvartab, echo=F}
# The next step is to rank order the p-values for each outcome set size. 
# The function to do this is defined in the initial block of functions
for (m in 1:length(methodlist)){
  method <- methodlist[m] 
  mypname <-paste0('p_1sided_method',method,'_allN_allES_allcorr_maxn',maxn,'_nsim',nsim,'_nstep',nstep) #filename to save csv file with simulated p-values (saves time when many runs used)
myfolder <-'data/'
  p.df<-read.csv(paste0(myfolder,mypname,'.csv'))

p.df <-rankp(p.df,outcomes)

#We now create a little table that gives the N outcomes that need to be p < .05 for overall FP rate to be maintained
#Here we focus only on cases where ES = 0, i.e. null hypothesis is true

shorttab<- data.frame(matrix(NA,nrow=length(mycorr),ncol=1+length(outcomes)))
colnames(shorttab) <-c('corr',paste0('N',outcomes)) #file to hold N significant
shorttab[,1]<-mycorr
for (j in 1:length(mycorr)){
  nullp <- filter(p.df,ES==0,corr==mycorr[j],Nsub==max(myN))
  for (i in 1:length(outcomes)){
    fprate <- vector() #initialise
    mysize <- outcomes[i]
    fcol <-  which(colnames(nullp)== paste0('r',mysize,".2"))
    fcols <- fcol:(fcol+mysize-2)
    
    for (f in fcols){
      thisfp <- length(which(nullp[,f]<.05))/nrow(nullp)
      fprate<-c(fprate,thisfp)
    } 
    w<-min(which(fprate<.05))
    w<-w+1 #start size is with 2 vars
    if (w>mysize){w<-mysize}
    shorttab[j,(i+1)]<-w
  }
}
write.csv(shorttab,paste0('data/MinNSig_method',method,'.csv'),row.names=F)
}
```


```{r powerForManyN,echo=F}
for (m in 1:length(methodlist)){
    method <- methodlist[m] 
mysname <-paste0('data/MinNSig_method',method,'.csv')
shorttab <- read.csv(mysname)
#Next we compute the proportion of all runs in each condition that give p < .05. This corresponds to familywise error rate when effect size is zero, and to power when effect size is > 0.shorttab <- read.csv('data/corrvars_Ns.csv')

#insert a column for the Nvar = 1 case
shorttab2 <- cbind(shorttab[,1],shorttab[2],shorttab[2:ncol(shorttab)])
colnames(shorttab2)[1:2]<-c('corr','N1')
shorttab2[,2]<-1

powertab<-cbind(shorttab[,1],shorttab[2],shorttab[2],shorttab2)
colnames(powertab)[1:3]<-c('ES','obsES','nsub')
N1col<-which(colnames(powertab)=="N1")

#add cols to hold data from PCs
lastcol<-ncol(powertab)
nucols <<- (lastcol+1):(lastcol+length(outcomes))
powertab[,nucols]<-NA
colnames(powertab)[nucols]<-paste0('PC',outcomes)

writerow <- 0
for (e in myES){
  for (n in myN){
    for (c in mycorr){
      tempdata<-dplyr::filter(p.df,ES==e,Nsub==n,corr==c)
      
      writerow <- writerow+1
      powertab[writerow,]<-NA
      powertab$ES[writerow]<-e
      powertab$nsub[writerow]<-n
      powertab$corr[writerow]<-c
      powertab$obsES[writerow]<-round(mean(tempdata$obsES),4)
      #put power for single variable (V1) in N1 column against corr = 0
      if(c==0){
        powertab$N1[writerow] <-length(which(tempdata$V1<.05))/nrow(tempdata)
      }
      for (v in outcomes){
        critcol <- which(colnames(shorttab2)==paste0('N',v))
        critrow <- which(shorttab2$corr==c)
        critN <-shorttab2[critrow,critcol] #N below .05 to control FP rate at .05
        thiscol<-paste0('r',v,'.',critN)
        w<-which(colnames(tempdata)==thiscol)
        power <- length(which(tempdata[,w]<.05))/nrow(tempdata)
        thatcol <- which(colnames(powertab)==paste0('N',v))
        powertab[writerow,thatcol]<-power
        
        PCcol <- which(colnames(tempdata)=='PC2')+v-1
        thispcp <- length(which(tempdata[,PCcol]<.05))/nrow(tempdata)
        powertab[writerow,(thatcol+length(outcomes))]<-thispcp
        
      }
    }
  }
}
# We can save this to disk, though it's not really necessary, as it can be created quickly

write.csv(powertab,paste0('data/powertab_method',method,'.csv'),row.names=F)
}
```







```{r makelongform,echo=F}

# For alternative plots, we need to convert to long form
# This is not currently used in the writeup; just retained for possible future use
powertab[,1]<-as.factor(powertab[,1])
powertab[,4]<-as.factor(powertab[,4])


power_long <- gather(powertab, Nvar, power, N1:PC12, factor_key=TRUE) #we'll put PCs in separate column later


#create new col for the PC values
#This is fiddly as we have to ignore the N1 rows, so have to find the right range for reading and writing
PCrange <- which(power_long$Nvar=="PC2")[1]:nrow(power_long) #first row of PCs to end
PCwrite <- which(power_long$Nvar=="N2")[1]:(which(power_long$Nvar=="N2")[1]+length(PCrange)-1)
power_long$PC<-NA #initialise
power_long$PC[PCwrite] <- power_long$power[PCrange]
power_long<-power_long[1:(PCrange[1]-1),]

w<-which(is.na(power_long$power)) #find those with no power value and drop
w1<-c(w,which(is.na(power_long$corr))) #find those with no corr value and drop
power_long <- power_long[-w1,]
```


```{r computepowergain,echo=F}
# Will help visualise effect if we compute gain over power when a single variable is used.
# We'll use computed power using pwr and observedES for N1, i.e. case with single outcome
power_long$powerbase<-NA
power_long$Nvar_gain<-NA
power_long$PC_gain<-NA
startrow<-which(power_long$Nvar=='N2')[1]
for (i in startrow:nrow(power_long)){
  n <- power_long$nsub[i]
  oe <- power_long$obsES[i]
  basepower <-pwr.t.test(n=n,d=oe,sig.level=.05,alternative='greater')$power
  #set power to greater as we have set d to be positive
  power_long$powerbase[i]<-basepower
  power_long$Nvar_gain[i]<-power_long$power[i]-power_long$powerbase[i]
  power_long$PC_gain[i]<-power_long$PC[i]-power_long$powerbase[i]
}


mypsumname <-paste0('p_1sided_method',method)
write.csv(power_long,paste0(myfolder,mypsumname,'.csv'),row.names=F)
```


```{r plots,echo=F}
# Now plot results to facilitate comparisons.
# facet plot for Nvar variation
# create variable with 'r = ' so labels are less confusing
power_long$rlabel <-as.factor(paste0('r = ',power_long$corr)) 
power_long$nsub2 <-as.factor(power_long$nsub)
p <- ggplot(power_long[-w1,], aes(obsES,Nvar_gain ))+
  geom_point(aes(colour = nsub2))+
  geom_hline(yintercept=0, linetype="dotted", color = "black")
p<-p + facet_grid(vars(Nvar),vars(rlabel))

#facet plot for PC
q_long <- power_long[power_long$Nvar!='N1',]
q_long$Nvar <- droplevels(q_long$Nvar)
q <- ggplot(q_long, aes(obsES,PC_gain ))+
  geom_point(aes(colour = nsub2))+
  geom_hline(yintercept=0, linetype="dotted", color = "black")

q <-q + facet_grid(vars(Nvar),vars(rlabel))


```


## Methods

Correlated variables were simulated using in the R programming language [@rcoreteam2020]. The script to generate and analyse simulated data is available on https://github.com/oscci/MinSigVar, and more technical details are provided in the Appendix. Two approaches to modeling correlated variables were compared, Method M and Method L.  

### Method for simulating outcomes
The _mvrnorm_ function of the _MASS_ package was used to generate a set of 12 outcome variables with a specified covariance matrix. For simplicity, all variables were simulated as random normal deviates with SD of 1, and the covariance matrix had a prespecified correlation, r, in all off-diagonal elements. The correlation varied across runs from 0 to .8 in steps of .2, and the number of simulated cases varied from 20 to 110 in steps of 30. Outcomes for Intervention (I) and Control (C) groups differed only in terms of the mean, which was always zero for the group C, and a given effect size, e, for group I. The average observed effect size for all measures in a given condition was computed, and used as the basis for comparisons of efficiency between single and multiple measure scenarios. 

This method is simple but potentially unrealistic because it is possible to have a set of outcomes that are independent of one another (r = 0) yet all having the same effect size, which is unlike real-world data. 

An alternative approach was evaluated, in which the set of 12 outcome measures are simulated as indicators of an underlying latent variable, which mediates the intervention effect.  This can be achieved by first simulating a latent variable, with an effect size of either zero, for group C, or e for group I. Observed outcome measures are then simulated as having a specific correlation with the latent variable - i.e. the correlation determines the extent to which the outcomes act as indicators of the latent variable. This can be achieved using the formula:  
- r * latentvar + sqrt(1-r^2)*err  

where r is the correlation between latentvar and each outcome, and latentvar is a vector of random normal deviates that is the same for each outcome variable, while err is a vector of random normal deviates that differs for each outcome variable. Note that when outcome variables are generated this way, the mean intercorrelation between them will be r^2^.  Thus if we want a set of outcome variables with mean intercorrelation of .4, we need to specify r in the formula above as sqrt(r) = .632. Furthermore, the effect size for the simulated variables will be lower than for the latent variable: to achieve an effect size, e, for the outcome variables, it is necessary to specify the effect size for the latent variable, e~l~, as e/r^2^. When this is done, the results with this method are closely similar to those obtained using MASS. The exception is for the case where r = 0, which is not computuable with this method - i.e. it is not possible to have a set of outcomes that are indicators of the same latent factor but which are uncorrelated. As noted above, the case where r = 0 is unrealistic in any case, and so for these simulations, the lowest value of r that was included was r = .2. 


```{r MethodLobsES,echo=F}
#check how obsES relates to trueES -expect to be same for Method M, lower for L
# [The ES and observed ES fall on a straight line  for Method M].
# This chunk is superseded - it was used when evaluating the latent variable method to check how effect sizes on outcome variables compared with the specified effect size. 
runme <- 0
if(runme ==1){
powertab<-read.csv('data/powertab_methodL.csv')
method='L'
fignum<-fignum+1
plot(powertab$ES,powertab$obsES,col=as.factor(powertab$corr),xlab='True ES in latent factor',ylab='Observed ES',main=paste0('Figure ',fignum,': Observed effect sizes with Method ',method))
abline(a=0,b=1,lty=2)
legend(.05,.6, legend=c('0','.2','.4','.6','.8'),title='correlation',
       col=1:5, lty=1, cex=0.8)
}

```

### Data reduction    
The size of the suite of outcome variables entered into later analysis ranged from 2 to 12. For each suite size, principal components were computed from data from the C and I groups combined, using the base R function _prcomp_ from the _stats_ package.  Thus PC2 is a principal component based on the first two outcome measures, PC4 based on the first four outcome measures, and so on.  
Power of analyses based on the principal components was compared with power obtained using the Adjust Nvar approach, as specified below. 


### Simulation parameters
1000 simulations were run for each combination of:  
- sample size per group, ranging from 20 to 110 in steps of 30  
- correlation between outcome variables, ranging from .2 to .8 in steps of .2  
- true effect size, taking values of 0, .3, .5, or .7.  
- Method to generate data, either M or L  

The data generated from each combination of conditions was used to derive results for different sizes of suites of outcome variables, ranging from 2 to 12. Thus, the analysis was first conducted on the first 2 outcome measures, then on the first 3 outcome measures, and so on. 

For each set of conditions, on each run, a one-tailed t-test was conducted to obtain a p-value for the comparison between C and I groups, assuming C would be lower. The p-values for outcome measures were rank ordered for each run and each suite size. 

### Identifying MinNSig
To obtain MinNSig, the results were filtered to include only the runs where the null hypothesis was true, i.e. effect size = 0. Then, the proportion of p-values less than .05 was calculated for each rank for each number of outcome variables, to find the highest rank at which the overall proportion was less than .05. This is the MinNSig.  
Table `r tabnum` gives a toy example of the logic, using the case where we have either 2 or 4 outcome measures. Columns V1 to V2 show p-values for the t-test comparing the two groups each of the 4 outcome measures. Columns r2.1 and r2.2 show the same p-values rank ordered for just the first two measures; columns r4.1 to r4.4 show the p-values rank ordered for all 4 outcomes. We can then count the number of p-values that are below .05 for all 1000 runs for each ranked position. With 2 outcomes, if we take just the first ranked (lowest) p-value, the probability of it being lower than .05 is around .10. For the 2nd ranked p-value, the probability drops below .05, to .002. Thus we set MinNSig to 2. 
We can then turn to the case where we have four outcomes: the probability of the lowest p-value being below .05 is .185; the probability of the second lowest being below .05 is .014. Thus again, we set MinNSig to 2. As noted above, when the correlation between variables is zero, we can use the binomial theorem to compute values in the final row; however, when variables are intercorrelated, more p-values will be below .05, and so MinNSig may be higher.  
Because MinNSig moves in quantum steps, the effective familywise error rate is often lower than .05. For instance, in the example above with a suite of four outcome measures, MinNSig is set to 2 when there is no correlation between outcomes, but this gives p = .014, rather than .05. 

```{r toydemo,echo=F}
# Read in some pre-simulated data.  NB the simulated data file is enormous and is *not* saved on github. It is created in script below, which may take a couple of hours to run  We use it here just for convenience
tabnum <- tabnum+1
p.df <- read.csv('data/p_1sided_methodM_allN_allES_allcorr_maxn12_nsim1000.csv')
nstep<-1
outcomes <- seq(2,12,nstep)
toybit <-filter(p.df,ES==0)
#need to create the ranks as they aren't saved with main file
toybit <- rankp(toybit[1:12,],outcomes)
wantcols<-c('V1','V2','V3','V4','r2.1','r2.2','r4.1','r4.2','r4.3','r4.4')
toybit <- toybit[,wantcols]
toybit<-round(toybit,3)
run <- c(1:10,999:1000)
toybit<-cbind(run,toybit)
dotrow <- rep('...',ncol(toybit))
toybit <- rbind(toybit[1:10,],dotrow,toybit[11:12,])
toybit[14,]<-c('N < .05',rep('.',4),100,2,185,14,0,0)
toybit[15,]<-c('p < .05',rep('.',4),.1,.002,.185,.014,0,0)
ft <- flextable(toybit)
ft<- fit_to_width(ft, max_width =8 )
mycaption <- paste0('Table ',tabnum,': Demonstration of how MinNSig is determined')
ft <- set_caption(ft,mycaption)
ft



```
### Computing power using Adjust Nvar
For each run of the simulation, and each number of outcome measures, we take the value of MinNSig from the previous step and compute the proportion of p-values below .05, depending on the effect size, sample size and correlation between measures. For effect sizes above zero, this proportion corresponds to the statistical power. 

Power using Adjust Nvar can be compared to:  
- power obtained with a single outcome measure for the same effect size and sample size  
- power obtained by using the principal component extracted for this set of outcome measures  


## Results

### MinNSig
`r tabnum <- tabnum+1`
Table `r tabnum` shows results from a simulation of the Adjust Nvar approach, with the values in the body of the table showing MinNSig, the minimum number of measures that would maintain the overall familywise error rate at 1 in 20, if each individual measure was evaluated at the significance criterion of .05. Because the t-test statistic used to determine p-values is adjusted for sample size, these values are independent of numbers of subjects. In principle, researchers could use Table `r tabnum` to specify in their research protocol the minimum number of outcomes that would reach their significance level in order for the null hypothesis to be rejected.

```{r tab-MinNSig,echo=F}
shorttabL<-read.csv('data/MinNSig_methodL.csv')
shorttabM<-read.csv('data/MinNSig_methodM.csv')
ft<-flextable(shorttabM)
mycap <- paste0('Table ',tabnum,'A: Values of MinNSig from Method M')
ft <- set_caption(ft,mycap)
ft<- fit_to_width(ft, max_width =8 )
ft
# ft2<-flextable(shorttabL)
# ft2<- fit_to_width(ft2, max_width =8 )
# mycap <- paste0('Table ',tabnum,'B: Values of MinNSig from Method L')
# ft2 <- set_caption(ft2,mycap)
# ft2


```

### Power of Adjust Nvar approach

`r fignum <- fignum+1`
Full tables of results for all combinations of parameters are provided in the Appendix. Figure `r fignum` plots power vs familywise error rate for different sizes of suite of outcome measures, including the case where a single outcome measure is used for comparison. 


```{r power-vs-fp, echo=F,warning =F}
# Function to plot power and familywise error rate so we can compare efficiency of different approaches
plotpower <- function(method){
  powertab<-read.csv(paste0('data/powertab_methodM.csv'))
  lastcol<-'N12'
  lastcolnum <- which(colnames(powertab)==lastcol)
  shortpower <- powertab[,1:lastcolnum]
  if(method=="P"){
      shortpower <- powertab[,c(1:5,(lastcolnum+1):ncol(powertab))]
  }

  effsize <- c('Null','Small','Medium','Large')
  fpcols <- 1:(length(myN)*length(mycorr)) #cols hold values for ES=0
  
  firstcol<-which(colnames(shortpower)=='N1')
  shortpower$colorcol <- shortpower$corr*10 #just a set of colors to code correlation levels
  #shortpower$colorcol[shortpower$colorcol==0]<-1 
  mycols<-unique(shortpower$colorcol)
  myshapes<-unique(shortpower$shapecol)
  shortpower$shapecol <- 1 #and a set of shapes to code Nsub
  shortpower$shapecol[shortpower$nsub==myN[2]]<-16
  shortpower$shapecol[shortpower$nsub==myN[3]]<-17
  shortpower$shapecol[shortpower$nsub==myN[4]]<-15
  mycols<-unique(shortpower$colorcol)
  myshapes<-unique(shortpower$shapecol)
  
  
 
  
  for (e in 2:length(myES)){  #ignore ES=0
       #compute N1 values using pwr
      solopower<-vector()
    for (n in myN){
  
     solopower <-c(solopower,pwr.t.test(n=n,d=myES[e],sig.level=.05,alternative='greater')$power)
    }
   solopower<-round(solopower,3)
    
    nfigrow <- round(((length(outcomes)+3)/3),0)
    par(mfrow=c(nfigrow,3))

    for (nvar in 1:length(outcomes)){
      xrange <- (e-1)*length(fpcols)+fpcols
      yrange <-fpcols
      xcol<-firstcol+nvar
      
      plot(shortpower[xrange,xcol],shortpower[yrange,xcol],main=paste0('Effect size: ',effsize[e],'  Nvar:',outcomes[nvar]),col=shortpower$colorcol,pch=shortpower$shapecol,xlim=c(0,1),ylim=c(.001,.1),log='y',xlab='Power',ylab='Familywise error rate (log scale)',cex=1.5)
      
      abline(v=.8,lty=3)
      abline(h=.05,lty=3)
      
    }
    #make plot for legend
    plot(NULL ,xaxt='n',yaxt='n',bty='n',ylab='',xlab='', xlim=0:1, ylim=0:1)
    legend("topleft", legend=c('.2','.4','.6','.8'),title='Correlation', pch=15, pt.cex=2.5, cex=1.2, bty='n', col = mycols[2:length(mycols)])
    legend("topright", legend=myN,title='N per group', pch=myshapes, pt.cex=2.5, cex=1.2, bty='n')
    text(.45,.15,paste0("Power for single outcome\n N = 20 (",solopower[1],"); N= 50 (",solopower[2],");\n N = 80 (",solopower[3],"); N = 110 (",solopower[4],")"))
  }
  
}
```

```{r doplotsM,echo=F,warning=F,fig.width=7,fig.asp=1.5,fig.cap='Figure 2'}
plotpower('M')

```

`r fignum <- fignum+1`
For these plots, the small, medium and large effect sizes correspond to Cohen's d of .3, .5 and .7. An efficient method is one that gives power of .8 or above, and a familywise error rate of .05 or less, i.e. the results should cluster in the bottom right quadrant. Power is dependent on sample size, as is evident from these plots where the unfilled figures, corresponding to the smallest sample size of 20 per group,fail to achieve power above .8, except at the highest effect size of .7.  It is evident from inspection that  the Adjust NVar approach compares favourably with the single outcome case, when there are 4 or more outcome measures in a suite.  The power tends to be at least as high as for the single variable case, with lower familywise error rates. 


  
Figure `r fignum` gives an equivalent plot for power from principal components 



```{r doplotsL,echo=F,warning=F,fig.width=7,fig.asp=1.5,fig.cap='Figure 4'}
plotpower('P')

```

The Principal Components from each suite of outcomes also give high levels of power, with Familywise error rate clustering around .05.  

## Discussion

The logic of conventional multiple testing is turned on its head with the Adjust Nvar approach, in that instead of adjusting the p-value used for significance (as in the Bonferroni correction, or methods based on False Discovery Rate), we adjust the number of individual outcome measures that we need to reach the intended significance criterion.  This value can be easily computed using the binomial theorem for a given suite size of outcomes if the measures are uncorrelated, but in the context of intervention trials that is an unrealistic assumption. 

One advantage of this approach is that it is more compatible with trials of interventions that are expected to affect a range of related processes, as is common in some fields such as education or speech and language therapy. In such cases, the need to specify a single primary outcome tends to create difficulties, because it is often unclear which of a suite of outcomes is likely to show an effect. Note that the Adjust Nvar approach does not give the researcher free rein to engage in p-hacking: the larger the suite of measures included in the study, the higher the value of MinNSig will be. It does, however, remove the need to put all one's eggs in one basket by pre-specifying one measure as the primary outcome. 

A second advantage of this approach is that once the values of MinNSig have been computed, it is very simple to apply, and could be used a priori to specify the criterion that will be used in a protocol, assuming the researcher already has a rough idea of the degree of intercorrelation between outcome measures. This may help guard against a tendency to explore different kinds of correction for multiple hypothesis testing only after viewing the data [@lazic2021].

A third advantage is that in effect, by including multiple outcome measures, one can improve the efficiency of a study, in terms of the trade-off between power and familywise errors. If several outcome measures are seen as imperfect proxy indicators of an underlying latent construct, then we are in effect building in a degree of within-study replication if we require that more than one measure shows the same effect in the same direction before we reject the null hypothesis. 

The results obtained with this approach depend crucially on assumptions embodied in the simulation that is used to derive predictions. Outcome measures simulated here are normally distributed, and quite uniform in their covariance structure.  It would be possible to generate datasets with different underlying covariance structures to be tested in the same way, but that is beyond the scope of this paper. 



# Appendix

Computed power for Adjust NVar method
```{r getpowertabsM,echo=F}
powerM<-read.csv('data/powertab_methodM.csv')

powerM$obsES<-round(powerM$obsES,3)
pcstart <- which(colnames(powerM)=='PC2')
powerMA <- powerM[,1:(pcstart-1)]
ft<-flextable(powerMA)
ft<- fit_to_width(ft, max_width =8 )
ft<-set_caption(ft,'Power table for MinSigVar Method (where ES = 0, values are familywise error rate)')
ft

powerMB <- powerM[,c(1:4,pcstart:ncol(powerM))]
ft2<-flextable(powerMB)
ft2<- fit_to_width(ft2, max_width =8 )
ft2<-set_caption(ft2,'Power table for principal components (where ES = 0, values are familywise error rate)')
ft2
```




# Notes
The script is available on https://github.com/oscci/MinSigVar.

# References


